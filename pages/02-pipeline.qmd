---
title: "Pipeline"
format:
  html:
    toc: true
---

Here we outline the work steps we followed to answer the questions in our problem statement. In the first subsection, we cover two initial setup tasks that prepare the data for the rest of the pipeline:

1. Translating CVs from Italian into English so we can apply semantic-matching techniques consistently. 
2. Labeling with an LLM each extracted skill as either a hard skill or a      soft skill.
   ğŸ”— [See the Hard Soft skills labelling notebook](../hiring_cv_bias/hard_soft_skills_labelling/hard_soft_skill_labelling.html)   

With these preparations in place, the pipeline we built is divided into the following steps:

1. **Data Cleaning**  
   We standardized and filtered the raw, anonymized CV texts with structural issues as:
   - Empty / Whitespace-Only/ Very Short Text
   - High Repetition / Low Structure
   - Low token & vocabulary Richness
   - Corrupted CVs with a fraction of â€œunusualâ€ characters
   - CVs with Placeholder Tails (e.g. â€œXXXXXXXXXXâ€¦â€)
   - CVs with Poor Translations

   Parsed skills of the candidates with corrupted CVs has been also excluded.

   ğŸ”— [See the Cleaning notebook](../notebooks/data_cleaning.html)

2. **Exploratory Analysis**  
   We explored basic statistics: total resumes per language, average length, and initial skill counts to understand data shape.  
   ğŸ”— [See the Exploration notebook](../notebooks/data_exploration.html)

3. **Distributions Analysis**  
   We plotted hard vs. soft skill frequencies, languageâ€based differences, and other distributions to spot obvious skews.  
   ğŸ”— [See the Distributions Analysis notebook](../notebooks/distributions_analysis.html)

4. **Bias Detection**  
   We applied groupâ€based comparisons (e.g. by inferred gender or language) to uncover where the parser over- or under-extracts certain skills.  
   ğŸ”— [See the Bias Detection notebook](../notebooks/bias_detection.html)

