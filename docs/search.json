[
  {
    "objectID": "notebooks/distributions_analysis.html",
    "href": "notebooks/distributions_analysis.html",
    "title": "Uncover distributional imbalances",
    "section": "",
    "text": "Main Objective:\nThis notebook aims to uncover distributional imbalances by combining demographic data (age, gender, location) from the Candidates sheet of the reverse matching dataset and skill data extracted by the parser from raw CVs."
  },
  {
    "objectID": "notebooks/distributions_analysis.html#steps",
    "href": "notebooks/distributions_analysis.html#steps",
    "title": "Uncover distributional imbalances",
    "section": "Steps:",
    "text": "Steps:\n\nAnalyze distributional skews\n\nGender distribution\nLocation distribution\n\nHard vs. soft skills distribution\n\nVisualize imbalances\n\nUse bar charts to highlight over or under representation.\n\nFirst analysis of parser induced bias\n\nIdentify patterns where the parser may systematically favor or overlook certain groups or skill types.\n\n\nWhy This Matters\n\nDetecting these imbalances is critical to designing a robust, fair pipeline that flags and mitigates biases introduced by the CV parser relying only on raw CV inputs and their parsed outputs."
  },
  {
    "objectID": "notebooks/distributions_analysis.html#merge-prepare-data",
    "href": "notebooks/distributions_analysis.html#merge-prepare-data",
    "title": "Uncover distributional imbalances",
    "section": "Merge & prepare data",
    "text": "Merge & prepare data\n\nBring together candidate demographic data and their extracted skills.\n\n\n%load_ext autoreload \n%autoreload 2\nimport polars as pl\n\nfrom hiring_cv_bias.config import (\n    CLEANED_REVERSE_MATCHING_PATH,\n    CLEANED_SKILLS,\n    HARD_SOFT_SKILLS,\n)\nfrom hiring_cv_bias.exploration.gender_analysis import (\n    add_zippia_columns,\n    compute_bias_strenght,\n    get_category_distribution,\n    get_skill_target_share,\n    plot_bias_skills_bar,\n)\nfrom hiring_cv_bias.exploration.utils import (\n    plot_distribution_bar,\n    split_df_per_attribute,\n)\nfrom hiring_cv_bias.exploration.visualize import (\n    compute_and_plot_disparity,\n    plot_histogram,\n    plot_target_distribution,\n)\nfrom hiring_cv_bias.utils import load_data\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ndf_skills = load_data(CLEANED_SKILLS)\ndf_info_candidates = load_data(CLEANED_REVERSE_MATCHING_PATH)\ndf_skills.head()\n\n\nshape: (5, 3)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\n\n\ni64\nstr\nstr\n\n\n\n\n7990324\n\"Computer Literacy\"\n\"IT_Skill\"\n\n\n7990324\n\"Dental Assistant (m/f)\"\n\"Job_title\"\n\n\n7990324\n\"General Labourer (other) (m/f)\"\n\"Job_title\"\n\n\n7990324\n\"Intern (m/f)\"\n\"Job_title\"\n\n\n7990324\n\"Italian\"\n\"Language_Skill\"\n\n\n\n\n\n\n\ndf_info_candidates.sample(5)\n\n\nshape: (5, 13)\n\n\n\nCANDIDATE_ID\nGender\nAge_bucket\nprofessional_categories_int\nmatterknowledges\nlanguages\nregulatedtrainings\ncandidatecity\ndrivinglicenses\nLONGITUDE\nLATITUDE\nexperience\nLocation\n\n\ni64\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nf64\nf64\nstr\nstr\n\n\n\n\n5837759\n\"Male\"\n\"25-34\"\n\"magazziniere/magazziniera\"\nnull\nnull\nnull\n\"MIRANO\"\nnull\n12.109877\n45.49277\n\"guardia del corpo/chauffeur pr…\n\"NORTH\"\n\n\n5920423\n\"Male\"\n\"25-34\"\n\"analista programmatore\"\nnull\nnull\nnull\n\"NAPOLI\"\nnull\n14.248768\n40.835885\n\"analista/programmatore;stagist…\n\"SOUTH\"\n\n\n5776720\n\"Male\"\n\"25-34\"\n\"Impiegato Commerciale\"\nnull\nnull\nnull\n\"VILLASANTA\"\nnull\n9.301397\n45.603924\n\"account director;impiegato tec…\n\"NORTH\"\n\n\n148015\n\"Male\"\n\"25-34\"\n\"Geometra e tecnico di costruzi…\n\"Microsoft Office;Google Chrome…\nnull\nnull\n\"PIEVE A NIEVOLE\"\n\"B;A\"\n10.796112\n43.879112\n\"responsabile di produzione e d…\n\"CENTER\"\n\n\n7036334\n\"Female\"\n\"55-74\"\n\"account manager\"\nnull\nnull\nnull\n\"RAVENNA\"\nnull\n12.201762\n44.416141\n\"addetto alle attività ricreati…\n\"CENTER\""
  },
  {
    "objectID": "notebooks/distributions_analysis.html#gender-analysis",
    "href": "notebooks/distributions_analysis.html#gender-analysis",
    "title": "Uncover distributional imbalances",
    "section": "Gender Analysis",
    "text": "Gender Analysis\nIn this section, we analyze the distribution of extracted skills across candidates by incorporating gender information.\nWe focus on:\n\nExploring the distribution of skills by gender.\nIdentifying job roles where the skill sets parsed from CVs exhibit significant gender based disparities.\nUncovering potential biases in how skills are emphasized for different genders.\n\n\ndf_skills_with_gender = df_skills.join(\n    df_info_candidates.select([\"CANDIDATE_ID\", \"Gender\"]), on=\"CANDIDATE_ID\"\n)\ndf_skills_with_gender.head()\n\n\nshape: (5, 4)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\nGender\n\n\ni64\nstr\nstr\nstr\n\n\n\n\n7990324\n\"Computer Literacy\"\n\"IT_Skill\"\n\"Female\"\n\n\n7990324\n\"Dental Assistant (m/f)\"\n\"Job_title\"\n\"Female\"\n\n\n7990324\n\"General Labourer (other) (m/f)\"\n\"Job_title\"\n\"Female\"\n\n\n7990324\n\"Intern (m/f)\"\n\"Job_title\"\n\"Female\"\n\n\n7990324\n\"Italian\"\n\"Language_Skill\"\n\"Female\"\n\n\n\n\n\n\n\ngender_counts_df = get_category_distribution(df_info_candidates, \"Gender\")\ngender_counts_df\n\n\nshape: (4, 3)\n\n\n\nGender\ncount\npercentage\n\n\nstr\nu32\nf64\n\n\n\n\n\"Male\"\n3984\n53.0\n\n\n\"Female\"\n3420\n45.5\n\n\n\"Other\"\n106\n1.4\n\n\n\"Unknown\"\n11\n0.1\n\n\n\n\n\n\nWe begin by examining the overall gender composition of the candidate pool, which shows 53.0% male and 45.5% female, therefore rather balanced and a small fraction identifying as “Other” or “Unknown.”\n\nplot_distribution_bar(\n    gender_counts_df,\n    x_col=\"Gender\",\n    y_col=\"count\",\n    x_label=\"Gender\",\n    y_label=\"Number of Candidates\",\n    title=\"Candidate Distribution by Gender\",\n)\n\n\n\n\n\n\n\n\nCompute for each skill type the counts and percentages of male vs. female candidates and their differences.\nSteps:\n1. Count males and females per skill type. These counts are normalized with respect to the prior distribution. 2. Calculate total count, percent female/male (rounded to 1 decimal), absolute and percentage differences.\n3. Sort by descending total count.\n\ndf_category_with_gender = get_skill_target_share(\n    df_skills_with_gender,\n    gender_counts_df,\n    target_col=\"Gender\",\n    target_values=[\"Male\", \"Female\"],\n)\ndf_category_with_gender\n\n\nshape: (5, 8)\n\n\n\nSkill_Type\ncount_male\ncount_female\ncount_total\nperc_male\nperc_female\ncount_diff\nperc_diff\n\n\nstr\ni64\ni64\ni64\nf64\nf64\ni64\nf64\n\n\n\n\n\"Professional_Skill\"\n32292\n34238\n66530\n48.5\n51.5\n-1946\n-3.0\n\n\n\"Job_title\"\n11171\n11643\n22814\n49.0\n51.0\n-472\n-2.0\n\n\n\"IT_Skill\"\n11282\n10628\n21910\n51.5\n48.5\n654\n3.0\n\n\n\"Language_Skill\"\n6199\n6584\n12783\n48.5\n51.5\n-385\n-3.0\n\n\n\"DRIVERSLIC\"\n1288\n1012\n2300\n56.0\n44.0\n276\n12.0\n\n\n\n\n\n\nWe identify skill categories with the gender imbalance, highlighting those that are disproportionately associated with either male or female candidates.\n\ngender_percs_dict = {\"Male\": \"perc_male\", \"Female\": \"perc_female\"}\ngender_colors = {\"Male\": \"skyblue\", \"Female\": \"lightcoral\"}\n\nplot_bias_skills_bar(\n    df_category_with_gender,\n    \"Skill_Type\",\n    gender_percs_dict,\n    \"perc_diff\",\n    \"Skill Categories gender imbalance\",\n    colors=gender_colors,\n)\n\n\n\n\n\n\n\n\nNow we analyze gender representation across parsed skills by computing both absolute counts (normalized by prior distribution) and relative percentages for male and female candidates. The objective is to identify skills that show a significant gender imbalance.\nWe group the data by each unique combination of Skill and Skill_Type and compute the following:\n\ncount_male: number of male candidates who have the skill\ncount_female: number of female candidates who have the skill\n\ncount_total = count_male + count_female\n\nperc_male = (count_male / count_total) × 100\n\nperc_female = (count_female / count_total) × 100\n\nperc_diff = perc_male - perc_female\n\ncount_diff = count_male - count_female\n\nTo quantify the strength of gender bias for each skill, we define the following metric:\n\\[\n\\text{bias\\_strength} = \\left| \\frac{\\text{count\\_diff}}{\\text{count\\_total}} \\cdot \\log(1 + \\text{count\\_total}) \\right|\n\\]\nThis formula combines: - the normalized difference between male and female counts, - a logarithmic weighting that increases confidence in imbalances occurring in larger samples.\nThe result is a score that emphasizes statistically meaningful disparities.\nA higher bias_strength indicates a stronger imbalance between male and female representations for that particular skill.\n\ndf_gender_bias = compute_bias_strenght(df_skills_with_gender, gender_counts_df)\ndf_gender_bias\n\n\nshape: (5_523, 10)\n\n\n\nSkill\nSkill_Type\ncount_male\ncount_female\ncount_total\nperc_male\nperc_female\ncount_diff\nperc_diff\nbias_strength\n\n\nstr\nstr\ni64\ni64\ni64\nf64\nf64\ni64\nf64\nf64\n\n\n\n\n\"Italian\"\n\"Language_Skill\"\n3086\n3085\n6171\n50.0\n50.0\n1\n0.0\n0.001414\n\n\n\"English\"\n\"Language_Skill\"\n1555\n1565\n3120\n49.8\n50.2\n-10\n-0.4\n0.025788\n\n\n\"Microsoft Office\"\n\"IT_Skill\"\n1301\n1529\n2830\n46.0\n54.0\n-228\n-8.0\n0.640365\n\n\n\"Driver License B\"\n\"DRIVERSLIC\"\n869\n872\n1741\n49.9\n50.1\n-3\n-0.2\n0.012859\n\n\n\"Microsoft Word\"\n\"IT_Skill\"\n777\n879\n1656\n46.9\n53.1\n-102\n-6.2\n0.456583\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Personal Injury Law\"\n\"Professional_Skill\"\n0\n1\n1\n0.0\n100.0\n-1\n-100.0\n0.693147\n\n\n\"Consultation Service\"\n\"Professional_Skill\"\n0\n1\n1\n0.0\n100.0\n-1\n-100.0\n0.693147\n\n\n\"Immunohaematology\"\n\"Professional_Skill\"\n0\n1\n1\n0.0\n100.0\n-1\n-100.0\n0.693147\n\n\n\"Baseball\"\n\"Professional_Skill\"\n0\n1\n1\n0.0\n100.0\n-1\n-100.0\n0.693147\n\n\n\"Forester (m/f)\"\n\"Job_title\"\n0\n1\n1\n0.0\n100.0\n-1\n-100.0\n0.693147\n\n\n\n\n\n\n\ndf_gender_bias.sort(pl.col(\"bias_strength\"), descending=True).head(20)\n\n\nshape: (20, 10)\n\n\n\nSkill\nSkill_Type\ncount_male\ncount_female\ncount_total\nperc_male\nperc_female\ncount_diff\nperc_diff\nbias_strength\n\n\nstr\nstr\ni64\ni64\ni64\nf64\nf64\ni64\nf64\nf64\n\n\n\n\n\"Baby-Sitter (m/f)\"\n\"Job_title\"\n3\n161\n164\n1.8\n98.2\n-158\n-96.4\n4.919143\n\n\n\"Programmable Logic Controllers\"\n\"Professional_Skill\"\n243\n21\n264\n92.0\n8.0\n222\n84.0\n4.692046\n\n\n\"Forklift Trucks\"\n\"Professional_Skill\"\n645\n112\n757\n85.2\n14.8\n533\n70.4\n4.668632\n\n\n\"Construction Worker (m/f)\"\n\"Job_title\"\n105\n0\n105\n100.0\n0.0\n105\n100.0\n4.663439\n\n\n\"Secretary (m/f)\"\n\"Job_title\"\n10\n174\n184\n5.4\n94.6\n-164\n-89.2\n4.652926\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Hydraulics\"\n\"Professional_Skill\"\n126\n14\n140\n90.0\n10.0\n112\n80.0\n3.959008\n\n\n\"Telephone Skills\"\n\"Professional_Skill\"\n3\n69\n72\n4.2\n95.8\n-66\n-91.6\n3.932921\n\n\n\"Room Attendant (m/f)\"\n\"Job_title\"\n0\n47\n47\n0.0\n100.0\n-47\n-100.0\n3.871201\n\n\n\"Credit Collections\"\n\"Professional_Skill\"\n9\n96\n105\n8.6\n91.4\n-87\n-82.8\n3.863992\n\n\n\"Plumber (m/f)\"\n\"Job_title\"\n45\n0\n45\n100.0\n0.0\n45\n100.0\n3.828641\n\n\n\n\n\n\n\nNote: From these results, we can see that certain skills known to be heavily “gender skewed” in society have been identified.  Although a high perc_diff highlights strong imbalances, it alone would also find rare skills with extreme ratios (for example, 1 occurrence versus 0). By adding a logarithmic term based on counts, we ensure that only skills with both a large percentage difference and a sufficiently high frequency rise to the top.  This bias_strenght metric therefore uncovers the most widespread, gender-biased skills in our dataset.\n\n\nplot_bias_skills_bar(\n    df_gender_bias,\n    \"Skill\",\n    gender_percs_dict,\n    \"bias_strength\",\n    \"Top Skills with Highest Gender Imbalance\",\n    top_n=20,\n    colors=gender_colors,\n)\n\n\n\n\n\n\n\n\nIn this section, we’re going to examine gender bias in the candidates job experiences (Job Titles) to understand how they differ for male and female candidates.\nFor doing this we append two new columns perc_female_zippia and perc_male_zippia by scraping Zippia (USA) for the percentage of men and women in each role.\nBy this we’ll see whether the same male/female proportions that we observe in our parsed skills and roles align with the real world distribution of those occupations determining whether observed disparities could reflect parser errors or real world biases already present in our CVs.\nExample: If our parsed CVs dataset shows that 10% of “Software Engineer” CVs are female but Zippia reports 30%, this gap may indicate a parser bias. Conversely, if both sources match closely, it could suggests that any skew is likely a reflection of broader societal patterns rather than a flaw in the parser extraction process.\n\ndf = df_gender_bias.sort(pl.col(\"bias_strength\"), descending=True).head(30)\njob_df = df.filter(pl.col(\"Skill_Type\") == \"Job_title\")\njob_df\n\n\nshape: (15, 10)\n\n\n\nSkill\nSkill_Type\ncount_male\ncount_female\ncount_total\nperc_male\nperc_female\ncount_diff\nperc_diff\nbias_strength\n\n\nstr\nstr\ni64\ni64\ni64\nf64\nf64\ni64\nf64\nf64\n\n\n\n\n\"Baby-Sitter (m/f)\"\n\"Job_title\"\n3\n161\n164\n1.8\n98.2\n-158\n-96.4\n4.919143\n\n\n\"Construction Worker (m/f)\"\n\"Job_title\"\n105\n0\n105\n100.0\n0.0\n105\n100.0\n4.663439\n\n\n\"Secretary (m/f)\"\n\"Job_title\"\n10\n174\n184\n5.4\n94.6\n-164\n-89.2\n4.652926\n\n\n\"Repair and Maintenance Technic…\n\"Job_title\"\n94\n2\n96\n97.9\n2.1\n92\n95.8\n4.384098\n\n\n\"Electrician (m/f)\"\n\"Job_title\"\n80\n1\n81\n98.8\n1.2\n79\n97.6\n4.297911\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Plumber (m/f)\"\n\"Job_title\"\n45\n0\n45\n100.0\n0.0\n45\n100.0\n3.828641\n\n\n\"Bricklayer (m/f)\"\n\"Job_title\"\n43\n0\n43\n100.0\n0.0\n43\n100.0\n3.78419\n\n\n\"Warehouse Employee (m/f)\"\n\"Job_title\"\n472\n122\n594\n79.5\n20.5\n350\n59.0\n3.764304\n\n\n\"Painter (m/f)\"\n\"Job_title\"\n42\n0\n42\n100.0\n0.0\n42\n100.0\n3.7612\n\n\n\"Carpenter (m/f)\"\n\"Job_title\"\n53\n2\n55\n96.4\n3.6\n51\n92.8\n3.732599\n\n\n\n\n\n\n\njob_df = add_zippia_columns(job_df)\njob_df\n\n\nshape: (15, 12)\n\n\n\nSkill\nSkill_Type\ncount_male\ncount_female\ncount_total\nperc_male\nperc_female\ncount_diff\nperc_diff\nbias_strength\nperc_female_zippia\nperc_male_zippia\n\n\nstr\nstr\ni64\ni64\ni64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Baby-Sitter (m/f)\"\n\"Job_title\"\n3\n161\n164\n1.8\n98.2\n-158\n-96.4\n4.919143\n86.0\n14.0\n\n\n\"Construction Worker (m/f)\"\n\"Job_title\"\n105\n0\n105\n100.0\n0.0\n105\n100.0\n4.663439\n6.0\n94.0\n\n\n\"Secretary (m/f)\"\n\"Job_title\"\n10\n174\n184\n5.4\n94.6\n-164\n-89.2\n4.652926\n90.0\n10.0\n\n\n\"Repair and Maintenance Technic…\n\"Job_title\"\n94\n2\n96\n97.9\n2.1\n92\n95.8\n4.384098\n4.0\n96.0\n\n\n\"Electrician (m/f)\"\n\"Job_title\"\n80\n1\n81\n98.8\n1.2\n79\n97.6\n4.297911\n4.0\n96.0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Plumber (m/f)\"\n\"Job_title\"\n45\n0\n45\n100.0\n0.0\n45\n100.0\n3.828641\n4.0\n96.0\n\n\n\"Bricklayer (m/f)\"\n\"Job_title\"\n43\n0\n43\n100.0\n0.0\n43\n100.0\n3.78419\n3.0\n97.0\n\n\n\"Warehouse Employee (m/f)\"\n\"Job_title\"\n472\n122\n594\n79.5\n20.5\n350\n59.0\n3.764304\n17.0\n83.0\n\n\n\"Painter (m/f)\"\n\"Job_title\"\n42\n0\n42\n100.0\n0.0\n42\n100.0\n3.7612\n14.0\n86.0\n\n\n\"Carpenter (m/f)\"\n\"Job_title\"\n53\n2\n55\n96.4\n3.6\n51\n92.8\n3.732599\n4.0\n96.0"
  },
  {
    "objectID": "notebooks/distributions_analysis.html#geographical-analysis",
    "href": "notebooks/distributions_analysis.html#geographical-analysis",
    "title": "Uncover distributional imbalances",
    "section": "Geographical Analysis",
    "text": "Geographical Analysis\nIn this section, we analyze the distribution of extracted skills across candidates by incorporating geographical position information.\nWe focus on:\n\nExploring the distribution of skills by geographical position.\nIdentifying job titles where the skill sets parsed from CVs exhibit significant position based disparities.\nUncovering potential biases in how skills are emphasized for different locations.\n\n\ndf_skill_candidates = df_info_candidates.join(\n    df_skills,\n    on=\"CANDIDATE_ID\",\n).select(\"CANDIDATE_ID\", \"LATITUDE\", \"Skill\", \"Skill_Type\")\ndisplay(df_skill_candidates)\n\n\nshape: (128_898, 4)\n\n\n\nCANDIDATE_ID\nLATITUDE\nSkill\nSkill_Type\n\n\ni64\nf64\nstr\nstr\n\n\n\n\n7990324\n44.575143\n\"Computer Literacy\"\n\"IT_Skill\"\n\n\n7990324\n44.575143\n\"Dental Assistant (m/f)\"\n\"Job_title\"\n\n\n7990324\n44.575143\n\"General Labourer (other) (m/f)\"\n\"Job_title\"\n\n\n7990324\n44.575143\n\"Intern (m/f)\"\n\"Job_title\"\n\n\n7990324\n44.575143\n\"Italian\"\n\"Language_Skill\"\n\n\n…\n…\n…\n…\n\n\n18233\n45.060169\n\"Knowledge of Finance\"\n\"Professional_Skill\"\n\n\n18233\n45.060169\n\"Maintenance\"\n\"Professional_Skill\"\n\n\n18233\n45.060169\n\"Multi-Level Marketing\"\n\"Professional_Skill\"\n\n\n18233\n45.060169\n\"Sales\"\n\"Professional_Skill\"\n\n\n18233\n45.060169\n\"Telecommunications\"\n\"Professional_Skill\"\n\n\n\n\n\n\nThis distribution reveals a pronounced geographic imbalance: with nearly three‐quarters of candidates (~71%) coming from the North and very few from the Center (~14%) or South (~15%); the pool is heavily skewed toward Northern regions.\nThis pronounced skew must be taken into account in all subsequent analyses.\n\ndf_skill_candidates_localized = df_skill_candidates.with_columns(\n    pl.when(pl.col(\"LATITUDE\") &gt; 44.5)\n    .then(pl.lit(\"NORTH\"))\n    .when(pl.col(\"LATITUDE\") &lt; 42)\n    .then(pl.lit(\"SOUTH\"))\n    .otherwise(pl.lit(\"CENTER\"))\n    .alias(\"Location\")\n)\ndf_location_per_candidate = df_skill_candidates_localized.select(\n    \"CANDIDATE_ID\", \"Location\"\n).unique()\nplot_histogram(\n    df_location_per_candidate[\"Location\"],\n    title=\"Candidates Geographical Distribution\",\n    normalize=True,\n)\n\n\n\n\n\n\n\n\nThe charts below display the percentage distribution of each Skill_Type within three geographic regions (North, Center, South). Each histogram is normalized, so differences in absolute CV counts (e.g., 71% of candidates coming from the North) do not affect the shape of the distribution across regions. The y-axis values represent the relative share of each skill type among CVs from that specific area, regardless of the total volume of CVs.\n\nNote: The only noticeable difference is that in the South, the Job_title category is slightly more prevalent than IT_Skill. In the North and Center, these two categories remain roughly similar. All other proportions (e.g., the dominance of Professional_Skill and the marginal share of DRIVERSLIC) are nearly identical across regions.\n\n\nskills_per_location = split_df_per_attribute(df_skill_candidates_localized, \"Location\")\nplot_target_distribution(skills_per_location, \"Geographical Skill Type Distribution\")\n\n\n\n\n\n\n\n\nWhen comparing how specific skills are distributed across multiple geographic regions, it is crucial to identify which skills exhibit the most pronounced imbalance. The technique employed here involves:\n\nCutting out low frequencies skills: first applying log trasformation on the total counts distribution, then computing z-score for each count and lastly filtering out those that are below a certain threshold.\nGathering frequency counts (already scaled considering the groups prior distribution) of each skill within each group. (considering only skills as described in 1.)\nQuantifying inequality for each skill across these groups using a statistical measure.\nSelecting the top skills with the maximum inequality and visualizing its breakdown to facilitate interpretation.\n\nThe Gini Index as an Inequality Metric\nThe chosen disparity metric is the Gini index, a widely used measure of statistical dispersion. For a given skill, let \\(n\\) be the number of groups and let \\(x_i\\) denote the frequency of that skill in group \\(i\\). We denote them by \\(x_{(1)}\\), \\(x_{(2)}\\), \\(...\\), \\(x_{(n)}\\). (in our case \\(n=3\\)) The Gini index (G) is then computed as:\n\\[\nG \\;=\\; \\frac{\\displaystyle\\sum_{1 \\,\\le i &lt; j \\,\\le n} \\bigl|x_i - x_j\\bigr|}{\\,n \\,\\sum_{i=1}^{n} x_i\\,}\\,.\n\\]\nHow this works:\n1. Intuition:\n- It measures the average absolute difference between every pair of group values, scaled by the total.\n- If all \\(x_i\\) are identical, each \\(\\lvert x_i - x_j\\rvert = 0\\), so \\(G=0\\) (perfect equality).\n- If one group has all of the mass and the others have zero, then the numerator is maximized, driving \\(G\\) toward 1 (maximal inequality).\n\nNormalization:\n\nDividing by \\(n \\sum_{i=1}^{n} x_i\\) ensures \\(G\\) ranges between 0 and (just under) 1 regardless of absolute scale or number of groups.\n\nIn practice, \\(G\\) approaches 1 when one group’s share dominates and the rest contribute negligibly.\n\nInterpretation:\n\nA low Gini (near 0) indicates the attribute is nearly equally represented across all groups.\n\nA high Gini signals that the attribute is concentrated in one or a few groups, revealing a strong disparity.\n\n\n\nNote: The Gini index’s maximum value is \\((n-1)/n\\). For \\(n=3\\), this gives a range from 0 up to \\(2/3\\) (approximately 0.667).\n\n\nprof_skills_per_location = {\n    attr: df.filter(pl.col(\"Skill_Type\") == \"Professional_Skill\")[\"Skill\"]\n    for attr, df in skills_per_location.items()\n}\n\n\nlocation_colors = {\"NORTH\": \"#2d8659\", \"CENTER\": \"#dddddd\", \"SOUTH\": \"#b03a2e\"}\n\nlocation_weights = {\n    key: 1 / len(df[\"CANDIDATE_ID\"].unique()) for key, df in skills_per_location.items()\n}\n\ncompute_and_plot_disparity(\n    prof_skills_per_location,\n    min_threshold=1.0,\n    colors=location_colors,\n    attribute_name=\"Professional_Skills\",\n    weights_dict=location_weights,\n)\n\n\n\n\n\n\n\n\n\nit_skills_per_location = {\n    attr: df.filter(pl.col(\"Skill_Type\") == \"IT_Skill\")[\"Skill\"]\n    for attr, df in skills_per_location.items()\n}\n\n\ncompute_and_plot_disparity(\n    it_skills_per_location,\n    colors=location_colors,\n    attribute_name=\"IT_Skills\",\n    weights_dict=location_weights,\n)\n\n\n\n\n\n\n\n\n\njob_title_per_location = {\n    attr: df.filter(pl.col(\"Skill_Type\") == \"Job_title\")[\"Skill\"]\n    for attr, df in skills_per_location.items()\n}\n\n\ncompute_and_plot_disparity(\n    job_title_per_location,\n    colors=location_colors,\n    attribute_name=\"Job_titles\",\n    weights_dict=location_weights,\n)\n\n\n\n\n\n\n\n\n\nlang_skills_per_location = {\n    attr: df.filter(pl.col(\"Skill_Type\") == \"Language_Skill\")[\"Skill\"]\n    for attr, df in skills_per_location.items()\n}\n\n\ncompute_and_plot_disparity(\n    lang_skills_per_location,\n    min_threshold=0.0,\n    colors=location_colors,\n    attribute_name=\"Language_Skill\",\n    weights_dict=location_weights,\n)\n\n\n\n\n\n\n\n\n\ndriverslic_per_location = {\n    attr: df.filter(pl.col(\"Skill_Type\") == \"DRIVERSLIC\")[\"Skill\"]\n    for attr, df in skills_per_location.items()\n}\n\n\ncompute_and_plot_disparity(\n    driverslic_per_location,\n    min_threshold=0.0,\n    colors=location_colors,\n    attribute_name=\"DRIVERSLIC\",\n    weights_dict=location_weights,\n)"
  },
  {
    "objectID": "notebooks/distributions_analysis.html#hard-soft-skills-analysis",
    "href": "notebooks/distributions_analysis.html#hard-soft-skills-analysis",
    "title": "Uncover distributional imbalances",
    "section": "Hard-Soft Skills Analysis",
    "text": "Hard-Soft Skills Analysis\nIn this section, we analyze the distribution of extracted skills across candidates by incorporating the hard/soft skills label.\nWe will see the relations between this label and the two areas already explored, with the aim to investigate better possible biases.\n\nhard_soft_skills = load_data(HARD_SOFT_SKILLS)\ndf_skills_with_label = df_skills.join(hard_soft_skills, on=\"Skill\")\ndf_skills_with_gender = df_skills_with_label.join(\n    df_info_candidates.select([\"CANDIDATE_ID\", \"Gender\"]), on=\"CANDIDATE_ID\"\n)\ndf_skills_with_gender\n\n\nshape: (67_873, 5)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\nlabel\nGender\n\n\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n7974050\n\"Billing Processes\"\n\"Professional_Skill\"\n\"Hard\"\n\"Female\"\n\n\n7974050\n\"Databases\"\n\"Professional_Skill\"\n\"Hard\"\n\"Female\"\n\n\n7974050\n\"Polling\"\n\"Professional_Skill\"\n\"Hard\"\n\"Female\"\n\n\n7965670\n\"Accounting\"\n\"Professional_Skill\"\n\"Hard\"\n\"Female\"\n\n\n7965670\n\"Administrative Operations\"\n\"Professional_Skill\"\n\"Soft\"\n\"Female\"\n\n\n…\n…\n…\n…\n…\n\n\n18233\n\"Knowledge of Finance\"\n\"Professional_Skill\"\n\"Hard\"\n\"Female\"\n\n\n18233\n\"Maintenance\"\n\"Professional_Skill\"\n\"Hard\"\n\"Female\"\n\n\n18233\n\"Multi-Level Marketing\"\n\"Professional_Skill\"\n\"Unknown\"\n\"Female\"\n\n\n18233\n\"Sales\"\n\"Professional_Skill\"\n\"Soft\"\n\"Female\"\n\n\n18233\n\"Telecommunications\"\n\"Professional_Skill\"\n\"Hard\"\n\"Female\"\n\n\n\n\n\n\nLet’s now see the gender distribution on this section of candidates.\n\ngender_counts_df = get_category_distribution(df_info_candidates, \"Gender\")\ngender_counts_df\n\n\nshape: (4, 3)\n\n\n\nGender\ncount\npercentage\n\n\nstr\nu32\nf64\n\n\n\n\n\"Male\"\n3984\n53.0\n\n\n\"Female\"\n3420\n45.5\n\n\n\"Other\"\n106\n1.4\n\n\n\"Unknown\"\n11\n0.1\n\n\n\n\n\n\n\ndf_gender_bias = get_skill_target_share(\n    df_skills_with_gender,\n    gender_counts_df,\n    target_col=\"Gender\",\n    target_values=[\"Male\", \"Female\"],\n    skill_col=[\"label\"],\n)\ndf_gender_bias\n\n\nshape: (3, 8)\n\n\n\nlabel\ncount_male\ncount_female\ncount_total\nperc_male\nperc_female\ncount_diff\nperc_diff\n\n\nstr\ni64\ni64\ni64\nf64\nf64\ni64\nf64\n\n\n\n\n\"Hard\"\n24441\n23143\n47584\n51.4\n48.6\n1298\n2.8\n\n\n\"Soft\"\n6435\n9787\n16222\n39.7\n60.3\n-3352\n-20.6\n\n\n\"Unknown\"\n1441\n1283\n2724\n52.9\n47.1\n158\n5.8\n\n\n\n\n\n\nAs we can see from the chart below, soft skills are prevalent for female candidates. (counts are normalized, as before, considering the prior distribution)\n\nplot_bias_skills_bar(\n    df_gender_bias,\n    \"label\",\n    gender_percs_dict,\n    \"perc_diff\",\n    \"Top Skills with Highest Gender Imbalance\",\n    colors=gender_colors,\n    figsize=(10, 6),\n)\n\n\n\n\n\n\n\n\n\nHard/Soft Skills: Geographical Analysis\n\ndf_skills_with_location = df_skills_with_label.join(\n    df_skill_candidates_localized.select(\"CANDIDATE_ID\", \"Location\", \"Skill\"),\n    on=[\"CANDIDATE_ID\", \"Skill\"],\n    coalesce=True,\n)\ndf_skills_with_location\n\n\nshape: (67_873, 5)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\nlabel\nLocation\n\n\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n7974050\n\"Billing Processes\"\n\"Professional_Skill\"\n\"Hard\"\n\"CENTER\"\n\n\n7974050\n\"Databases\"\n\"Professional_Skill\"\n\"Hard\"\n\"CENTER\"\n\n\n7974050\n\"Polling\"\n\"Professional_Skill\"\n\"Hard\"\n\"CENTER\"\n\n\n7965670\n\"Accounting\"\n\"Professional_Skill\"\n\"Hard\"\n\"NORTH\"\n\n\n7965670\n\"Administrative Operations\"\n\"Professional_Skill\"\n\"Soft\"\n\"NORTH\"\n\n\n…\n…\n…\n…\n…\n\n\n18233\n\"Knowledge of Finance\"\n\"Professional_Skill\"\n\"Hard\"\n\"NORTH\"\n\n\n18233\n\"Maintenance\"\n\"Professional_Skill\"\n\"Hard\"\n\"NORTH\"\n\n\n18233\n\"Multi-Level Marketing\"\n\"Professional_Skill\"\n\"Unknown\"\n\"NORTH\"\n\n\n18233\n\"Sales\"\n\"Professional_Skill\"\n\"Soft\"\n\"NORTH\"\n\n\n18233\n\"Telecommunications\"\n\"Professional_Skill\"\n\"Hard\"\n\"NORTH\"\n\n\n\n\n\n\n\nlocation_counts_df = get_category_distribution(\n    df_skill_candidates_localized.unique(\"CANDIDATE_ID\"), \"Location\"\n)\nlocation_counts_df\n\n\nshape: (3, 3)\n\n\n\nLocation\ncount\npercentage\n\n\nstr\nu32\nf64\n\n\n\n\n\"NORTH\"\n5397\n71.8\n\n\n\"SOUTH\"\n1098\n14.6\n\n\n\"CENTER\"\n1026\n13.6\n\n\n\n\n\n\n\ndf_location_bias = get_skill_target_share(\n    df_skills_with_location,\n    location_counts_df,\n    target_col=\"Location\",\n    target_values=[\"NORTH\", \"CENTER\", \"SOUTH\"],\n    skill_col=[\"label\"],\n)\ndf_location_bias\n\n\nshape: (3, 10)\n\n\n\nlabel\ncount_north\ncount_center\ncount_south\ncount_total\nperc_north\nperc_center\nperc_south\ncount_diff\nperc_diff\n\n\nstr\ni64\ni64\ni64\ni64\nf64\nf64\nf64\ni64\nf64\n\n\n\n\n\"Hard\"\n21928\n12850\n13754\n48532\n45.2\n26.5\n28.3\n5449\n11.3\n\n\n\"Soft\"\n7158\n4269\n5150\n16577\n43.2\n25.8\n31.1\n1338\n8.1\n\n\n\"Unknown\"\n1280\n723\n761\n2764\n46.3\n26.2\n27.5\n346\n12.5\n\n\n\n\n\n\nAs shown in the chart below, the Northern bars are consistently the tallest, indicating that candidates from the North have, on average, more skills.\n\nlocation_percs_dict = {\n    \"NORTH\": \"perc_north\",\n    \"CENTER\": \"perc_center\",\n    \"SOUTH\": \"perc_south\",\n}\n\nplot_bias_skills_bar(\n    df_location_bias,\n    \"label\",\n    location_percs_dict,\n    \"perc_diff\",\n    \"Top Skills with Highest Location Imbalance\",\n    colors=location_colors,\n    figsize=(10, 6),\n)"
  },
  {
    "objectID": "notebooks/data_cleaning.html",
    "href": "notebooks/data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Main Objective:\nIn this notebook we inspect and clean missing or duplicate values in the three main datasets:\n%load_ext autoreload \n%autoreload 2\n\nimport polars as pl\n\nfrom hiring_cv_bias.cleaning.common import (\n    cramers_v,\n    filter_out_candidate_ids,\n    find_dropped_skill_rows,\n    inspect_missing,\n    plot_cramer_matrix,\n)\nfrom hiring_cv_bias.cleaning.raw_cv import (\n    add_length_column,\n    assess_translation_completeness,\n    detect_corrupted_cvs,\n    detect_repetitive_cvs,\n    detect_vocab_sparsity,\n    filter_placeholder_tails,\n    find_and_print_short_cvs,\n    is_this_language,\n    plot_length_histogram,\n)\nfrom hiring_cv_bias.config import (\n    CANDIDATE_CVS_TRANSLATED_PATH,\n    PARSED_DATA_PATH,\n    REVERSE_MATCHING_PATH,\n)\nfrom hiring_cv_bias.utils import (\n    filter_unknown_and_other_rows,\n    load_data,\n    load_excel_sheets,\n)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "notebooks/data_cleaning.html#raw-cvs-cleaning-steps",
    "href": "notebooks/data_cleaning.html#raw-cvs-cleaning-steps",
    "title": "Data Cleaning",
    "section": "1. Raw CVs Cleaning Steps",
    "text": "1. Raw CVs Cleaning Steps\n\n1. Schema & Load\n\nDefine expected columns and data types (CANDIDATE_ID: UInt64, CV_text_anon: String, Translated_CV: String)\n\nLoad CSV into Polars DataFrame, verify row count matches the number of uniques Candidate IDs.\nCheck for duplicate CANDIDATE_ID entries -&gt; no duplicates found.\n\n\nraw_cv = load_data(CANDIDATE_CVS_TRANSLATED_PATH)\nprint(\"DataFrame schema:\", raw_cv.schema)\nprint(f\"Loaded: {raw_cv.height} CVs\")\n\n################\n\ntotal_rows = raw_cv.height\nunique_ids = raw_cv.select(pl.col(\"CANDIDATE_ID\")).unique().height\nprint(f\"Unique CANDIDATE_ID: {unique_ids}\")\ndisplay(raw_cv.sample(5))\n\nDataFrame schema: Schema({'CANDIDATE_ID': Int64, 'CV_text_anon': String, 'Translated_CV': String})\nLoaded: 7769 CVs\nUnique CANDIDATE_ID: 7769\n\n\n\nshape: (5, 3)\n\n\n\nCANDIDATE_ID\nCV_text_anon\nTranslated_CV\n\n\ni64\nstr\nstr\n\n\n\n\n4376747\n\"Curriculum vitae INFORMAZIONI …\n\"curriculum vitae personal info…\n\n\n6479915\n\"CV anonimizzato: F O R M A T …\n\" European format for CV person…\n\n\n6290282\n\"CV anonimizzato: \"\"\"       C U…\n\" curriculum vitae personal inf…\n\n\n6109581\n\"CV anonimizzato: \"\"\"       DA…\n\" personal data born on 06.03.1…\n\n\n6869415\n\"CV anonimizzato: \"\"\" Formato e…\n\" European format for CV person…\n\n\n\n\n\n\n\n\n2. Missing Value Inspection\n\nCount nulls per column (null_count()), compute percentage of missing values and report any columns with &gt; 0% missing\n\n\nmissing_stats = inspect_missing(raw_cv)\n\nMissing value summary:\n\n\n\n\nshape: (3, 3)\n\n\n\ncolumn\nn_missing\npct_missing\n\n\nstr\nu32\nf64\n\n\n\n\n\"CANDIDATE_ID\"\n0\n0.0\n\n\n\"CV_text_anon\"\n0\n0.0\n\n\n\"Translated_CV\"\n0\n0.0\n\n\n\n\n\n\n\nNo missing values detected in any column.\n\n\n\n\n3. Short- or Empty-CV Detection\nUnder this step we will:\n\nEmpty / Whitespace-Only/ Very Short Text\n\nFilter out any records where CV_text_anon is below a chosen threshold (e.g. 300 characters), showing their IDs and snippets for manual review.\n\nHigh Repetition / Low Structure\n\nSplit each CV into non-empty lines, count n_lines and unique_lines.\n\nCompute repetition_ratio = 1 − (unique_lines / n_lines).\n\nFlag and drop any CVs with repetition_ratio above a threshold (e.g. &gt; 0.7).\n\n\nThese checks ensure we remove any CVs that are dominated by boilerplate before proceeding with further analysis.\n\nplot_length_histogram(raw_cv, text_col=\"CV_text_anon\", bin_size=300, max_bin=3000)\n\n\n\n\n\n\n\n\nRemove Empty / Whitespace-Only/ Very Short Text\n\nraw_cv = add_length_column(raw_cv, text_col=\"CV_text_anon\", length_col=\"len_anon\")\n\n\ntoo_short_df = find_and_print_short_cvs(\n    raw_cv,\n    length_col=\"len_anon\",\n    threshold=300,\n    id_col=\"CANDIDATE_ID\",\n    text_col=\"CV_text_anon\",\n)\n\nFound 77 CVs with CV_text_anon &lt; 300 chars. Showing up to 10:\n 1. ID 7560889: 263 chars\n 2. ID 7456101: 29 chars\n 3. ID 7437776: 279 chars\n 4. ID 7414638: 110 chars\n 5. ID 7373536: 58 chars\n 6. ID 7364629: 202 chars\n 7. ID 7347338: 82 chars\n 8. ID 7330315: 237 chars\n 9. ID 7230306: 238 chars\n 10. ID 7163148: 253 chars\n\nExample full CV for ID 6562147:\n'CV anonimizzato:\\n\"\"\"\\nINFORMAZIONI DI\\nIndirizzo : Via Kuliscio\\n\"\"\"'\n\n\n\nshort_cvs_ids = too_short_df.select(pl.col(\"CANDIDATE_ID\")).to_series().to_list()\nraw_cv_cleaned = filter_out_candidate_ids(\n    raw_cv, short_cvs_ids, df_name=\"CVs\", description=\"under 300 characters\"\n)\n\nCVs with under 300 characters: 77 out of 7769 total\nThat is 0.99% of all cvs\nCVs -&gt; Original rows: 7769\nCVs -&gt; Dropped rows:  77 (IDs in provided list)\nCVs -&gt; Remaining rows: 7692\n\n\nRemove High Repetition / Low Structure CVs\n\nrepetitive_cvs = detect_repetitive_cvs(\n    raw_cv_cleaned, text_col=\"CV_text_anon\", max_repetition=0.5\n)\nprint(f\"Found {repetitive_cvs.height} repetitive CVs:\")\ndisplay(repetitive_cvs.select([\"CANDIDATE_ID\", \"n_lines\", \"repetition_ratio\"]))\n\nFound 39 repetitive CVs:\n\n\n\nshape: (39, 3)\n\n\n\nCANDIDATE_ID\nn_lines\nrepetition_ratio\n\n\ni64\ni64\nf64\n\n\n\n\n7259455\n244\n0.553279\n\n\n7026944\n105\n0.971429\n\n\n6981854\n134\n0.835821\n\n\n6876738\n195\n0.548718\n\n\n6788013\n103\n0.514563\n\n\n…\n…\n…\n\n\n164616\n401\n0.992519\n\n\n153773\n58\n0.706897\n\n\n125099\n172\n0.523256\n\n\n122003\n205\n0.965854\n\n\n121482\n151\n0.695364\n\n\n\n\n\n\n\nrepetitive_cvs_ids = repetitive_cvs.select(pl.col(\"CANDIDATE_ID\")).to_series().to_list()\nraw_cv_cleaned = filter_out_candidate_ids(\n    raw_cv_cleaned, repetitive_cvs_ids, df_name=\"CVs\", description=\"repetitive CVs\"\n)\n\nCVs with repetitive CVs: 39 out of 7692 total\nThat is 0.51% of all cvs\nCVs -&gt; Original rows: 7692\nCVs -&gt; Dropped rows:  39 (IDs in provided list)\nCVs -&gt; Remaining rows: 7653\n\n\n\n\n4. Text Quality Checks\nTo further ensure we filter out “bad” or malformed CVs, we will perform:\n\nToken & Vocabulary Richness\n\nCalculate the unique words / total words ratio to measure lexical variety.\nFilter out CVs with very low number of unique words (e.g &lt; 20) or very low ratio (e.g. &lt; 0.2).\n\nDetect Redacted‐Placeholder Tails\n\nFlag any CV whose text ends with a long run of the same character (e.g. “XXXXXXXXXX…”), seen after manual inspection. These indicate fully redacted templates with no usable content and should be excluded.\n\nDetect Garbled/Corrupted CVs\n\nCompute the fraction of “unusual” characters (outside printable ASCII, Latin-1, or standard punctuation) in each CV. Flag and remove any CV where this fraction exceeds a small threshold (e.g. 3%), catching heavily garbled or control-code–laden documents.\n\n\nThese additional checks will help us catch CVs that are too short, overly repetitive, structurally invalid, or otherwise unfit for reliable parsing and downstream analysis.\nToken & Vocabulary Richness\n\nsparse_cvs = detect_vocab_sparsity(\n    raw_cv_cleaned, text_col=\"CV_text_anon\", min_words=30, min_ttr=0.3\n)\n\nprint(f\"CVs to discard based on vocabulary sparsity: {sparse_cvs.height}\")\ndisplay(\n    sparse_cvs.select([\"CANDIDATE_ID\", \"total_words\", \"unique_words\", \"ttr\"]).sort(\n        pl.col(\"ttr\"), descending=False\n    )\n)\n\nCVs to discard based on vocabulary sparsity: 48\n\n\n\nshape: (48, 4)\n\n\n\nCANDIDATE_ID\ntotal_words\nunique_words\nttr\n\n\ni64\ni64\ni64\nf64\n\n\n\n\n6206994\n1291\n242\n0.187452\n\n\n7080072\n755\n149\n0.197351\n\n\n6015186\n931\n191\n0.205156\n\n\n6809297\n462\n101\n0.218615\n\n\n6987708\n1621\n375\n0.231339\n\n\n…\n…\n…\n…\n\n\n6501990\n17\n16\n0.941176\n\n\n4479547\n17\n16\n0.941176\n\n\n3583383\n22\n21\n0.954545\n\n\n6056802\n27\n26\n0.962963\n\n\n6099154\n14\n14\n1.0\n\n\n\n\n\n\n\nsparse_ids = sparse_cvs.select(pl.col(\"CANDIDATE_ID\")).to_series().to_list()\nraw_cv_cleaned = filter_out_candidate_ids(\n    raw_cv_cleaned, sparse_ids, df_name=\"CVs\", description=\"low lexical variety\"\n)\n\nCVs with low lexical variety: 48 out of 7653 total\nThat is 0.63% of all cvs\nCVs -&gt; Original rows: 7653\nCVs -&gt; Dropped rows:  48 (IDs in provided list)\nCVs -&gt; Remaining rows: 7605\n\n\nDetect Redacted‐Placeholder Tails\n\nplaceholder_cvs = filter_placeholder_tails(\n    raw_cv_cleaned, text_col=\"CV_text_anon\", char=\"X\", min_run=20\n)\nprint(f\"Found {placeholder_cvs.height} CVs with trailing X placeholders:\")\ndisplay(placeholder_cvs.select([\"CANDIDATE_ID\"]))\n\nFound 52 CVs with trailing X placeholders:\n\n\n\nshape: (52, 1)\n\n\n\nCANDIDATE_ID\n\n\ni64\n\n\n\n\n7471471\n\n\n7423990\n\n\n7386710\n\n\n7295445\n\n\n7228164\n\n\n…\n\n\n166297\n\n\n156390\n\n\n151354\n\n\n146771\n\n\n118642\n\n\n\n\n\n\n\nplaceholder_ids = placeholder_cvs.select(pl.col(\"CANDIDATE_ID\")).to_series().to_list()\nraw_cv_cleaned = filter_out_candidate_ids(\n    raw_cv_cleaned,\n    placeholder_ids,\n    df_name=\"CVs\",\n    description=\"trailing X placeholders\",\n)\n\nCVs with trailing X placeholders: 52 out of 7605 total\nThat is 0.68% of all cvs\nCVs -&gt; Original rows: 7605\nCVs -&gt; Dropped rows:  52 (IDs in provided list)\nCVs -&gt; Remaining rows: 7553\n\n\nDetect Garbled/Corrupted CVs\n\ncorrupted_cvs = detect_corrupted_cvs(\n    raw_cv_cleaned, text_col=\"CV_text_anon\", max_unusual_frac=0.02\n)\nprint(f\"Corrupted CVs to discard: {corrupted_cvs.height}\")\ndisplay(\n    corrupted_cvs.select([\"CANDIDATE_ID\", \"unusual_frac\"]).sort(\n        \"unusual_frac\", descending=True\n    )\n)\n\nCorrupted CVs to discard: 14\n\n\n\nshape: (14, 2)\n\n\n\nCANDIDATE_ID\nunusual_frac\n\n\ni64\nf64\n\n\n\n\n7265646\n0.71831\n\n\n7442026\n0.683837\n\n\n2963734\n0.218973\n\n\n4624374\n0.044884\n\n\n6745849\n0.043478\n\n\n…\n…\n\n\n7242049\n0.021448\n\n\n7133277\n0.021265\n\n\n6899819\n0.021021\n\n\n5400351\n0.020385\n\n\n6056462\n0.020049\n\n\n\n\n\n\n\ncorrupted_ids = corrupted_cvs.select(pl.col(\"CANDIDATE_ID\")).to_series().to_list()\nraw_cv_cleaned = filter_out_candidate_ids(\n    raw_cv_cleaned, corrupted_ids, df_name=\"CVs\", description=\"corrupted symbols\"\n)\n\nCVs with corrupted symbols: 14 out of 7553 total\nThat is 0.19% of all cvs\nCVs -&gt; Original rows: 7553\nCVs -&gt; Dropped rows:  14 (IDs in provided list)\nCVs -&gt; Remaining rows: 7539\n\n\n\n\n5. Translation Completeness\n\nFilter Out Poor Translations\nCompute len_ratio = len(Translated_CV) / len(CV_text_anon) and flag any CV with  len_ratio &lt; 0.7 (e.g. single-character outputs or garbled text) or an empty/missing translation. \n\nSince these cases are very rare, we drop them outright instead of attempting a fallback or re-translation.\n\ntranslation_stats_df = assess_translation_completeness(raw_cv_cleaned)\ndisplay(\n    translation_stats_df.filter(pl.col(\"len_ratio\") &lt; 0.7)\n    .select([\"CANDIDATE_ID\", \"orig_len\", \"trans_len\", \"len_ratio\"])\n    .sort(\"len_ratio\", descending=False)\n)\n\n\nshape: (18, 4)\n\n\n\nCANDIDATE_ID\norig_len\ntrans_len\nlen_ratio\n\n\ni64\ni64\ni64\nf64\n\n\n\n\n7164534\n6328\n107\n0.016909\n\n\n158927\n2700\n139\n0.051481\n\n\n127945\n1736\n798\n0.459677\n\n\n6661398\n3130\n1731\n0.553035\n\n\n7303615\n1134\n661\n0.582892\n\n\n…\n…\n…\n…\n\n\n5964952\n2017\n1325\n0.656916\n\n\n5871851\n5450\n3636\n0.667156\n\n\n6278055\n330\n223\n0.675758\n\n\n1009832\n4821\n3332\n0.691143\n\n\n6200024\n1442\n997\n0.691401\n\n\n\n\n\n\n\nlow_translation_ids = (\n    translation_stats_df.filter(pl.col(\"len_ratio\") &lt; 0.7)\n    .select(\"CANDIDATE_ID\")\n    .to_series()\n    .to_list()\n)\nraw_cv_cleaned = filter_out_candidate_ids(\n    raw_cv_cleaned,\n    low_translation_ids,\n    df_name=\"CVs\",\n    description=\"low translation completeness\",\n)\n\nCVs with low translation completeness: 18 out of 7539 total\nThat is 0.24% of all cvs\nCVs -&gt; Original rows: 7539\nCVs -&gt; Dropped rows:  18 (IDs in provided list)\nCVs -&gt; Remaining rows: 7521\n\n\nWe ran our language‐detection check on the Italian CV_text_anon and found 155 records flagged as “not Italian.”\nSince our downstream analysis relies exclusively on the English translations, we’re not going to drop these files. Instead, we will now verify that the Translated_CV column truly contains English text before proceeding with bias and skill‐extraction analyses.\n\nnot_italian_df = (\n    raw_cv_cleaned.with_columns(\n        [\n            pl.col(\"CV_text_anon\")\n            .map_elements(\n                lambda s, *_: not is_this_language(s or \"\", \"it\"),\n                return_dtype=pl.Boolean,\n            )\n            .alias(\"not_italian\")\n        ]\n    )\n    .select([\"CANDIDATE_ID\", \"CV_text_anon\", \"not_italian\"])\n    .filter(pl.col(\"not_italian\"))\n)\nprint(f\"Found {not_italian_df.height} CVs not in Italian.\")\ndisplay(not_italian_df.head())\n\nFound 153 CVs not in Italian.\n\n\n\nshape: (5, 3)\n\n\n\nCANDIDATE_ID\nCV_text_anon\nnot_italian\n\n\ni64\nstr\nbool\n\n\n\n\n7931173\n\"CV anonimizzato: \"\"\" PERSONAL …\ntrue\n\n\n7927173\n\"CV anonimizzato: \"\"\"       PER…\ntrue\n\n\n7926185\n\"CV: \"\"\" E U R O P E A N C U R …\ntrue\n\n\n7734475\n\"CV anonimizzato: \"\"\" Content D…\ntrue\n\n\n7560905\n\"CV anonimizzato: \"\"\" PROFILE L…\ntrue\n\n\n\n\n\n\n\nprint(\"Sample CVs not in Italian:\")\nprint(not_italian_df.sample(1)[\"CV_text_anon\"].item())\n\nSample CVs not in Italian:\nCV:\n\"\"\"\nEDUCATION\nLiceo E. Fermi (Cantù – Italy)\nDiploma: “Maturità scientifica” completed in 1984. \nWORK EXPERIENCE\n    April 2016\n    -\n    Present\nAdotta : company leader in the market for production of office furniture (Partition walls/ furniture and equipped workstation). \nPosition: Sales Manager , in charge of Foreign Customers, managing 7 resources, Based in Washington DC office. Main Duties:\nDefinition of export strategies and sales plans;\nConsolidation of exiting relationships with customer in American, Canada, Central America and South American markets; \nManagement of communication flows with the customer for information on various orders and their progress;\nAccurate assessment of internal and external logistics for the organization of shipments, resolution of issues concerning goods delivery.\n    December 2015\n    -\n    March 2016\nIVM S.p.A.: company leader in the market for production of office furniture (Partition walls/ furniture and equipped workstation). \nPosition: Export Manager.\nOctober 1999\n-\n    November 2015\nCitterio S .p.A. ( Molteni & C. SPA group): company leader in the production of office furniture (Partition walls/furniture and equipped workstations).\nPositions: Export Manager, in charge of Italian and Foreign Customers, managing three resources. Main duties:\nDefinition of export strategies and sales plans;\nFostering relationships with major customers, distributors and agents;\nConsolidation of existing relationships with customers in the European, American, Asian, Middle-Eastern and Australian markets, providing them with the necessary technical and commercial assistance;\nCreation and organization of a network of agents and dealers, providing the necessary support and training\nControl of customer credit lines and any guarantees required;\nManagement of communication flows with the customer for information on various orders and their progress;\nAccurate assessment of internal and external logistics for the organization of shipments, resolution of issues concerning goods delivery.\nSeptember 1994\n-\nSeptember 1999\nAll Over S.r.l. designs studio for printed fabrics\nPosition: Sales Manager. Main duties:\nexcellent experience in entertaining relationships with major Italian customers;\nopening new markets in the Middle and Far East, increasing the development and growth of the turnover;\nConsolidation of relations with existing customers in the European, American markets, providing them with the necessary technical and commercial assistance;\nCreation and organization of a network of agents and dealers, providing the necessary support and training.\nFebruary 1992\n-\nSeptember 1994\nElle 2 S.r.l. designs studio for printed fabrics\nPosition: Area Manager for the Italian, European and Japanese markets. \nUntil\nFebruary 1992\nSales agent for publishing products and software.\n    FOREIGN LANGUAGES\nEnglish (fluent), Spanish (fluent), French (good). \n    COMPUTER SKILLS\nGood experience in computer use and knowledge of the most used office automation softwares (Windows 10, MS Word , MS Excel, MS Power Point, AS400, AutoCAD 2015) .\n    OTHER \n    SKILLS\nExcellent suitability to human co ntacts and teamwork.\nFlexibility and orientation to problem solving.\nAbility to work on targets and in stressful situations.\n\"\"\"\n\n\n\nnot_english_df = (\n    raw_cv_cleaned.with_columns(\n        [\n            pl.col(\"Translated_CV\")\n            .map_elements(\n                lambda s, *_: not is_this_language(s or \"\", \"en\"),\n                return_dtype=pl.Boolean,\n            )\n            .alias(\"not_english\")\n        ]\n    )\n    .select([\"CANDIDATE_ID\", \"Translated_CV\", \"not_english\"])\n    .filter(pl.col(\"not_english\"))\n)\nprint(f\"Found {not_english_df.height} CVs not in English.\")\ndisplay(not_english_df.head())\n\nFound 16 CVs not in English.\n\n\n\nshape: (5, 3)\n\n\n\nCANDIDATE_ID\nTranslated_CV\nnot_english\n\n\ni64\nstr\nbool\n\n\n\n\n7546519\n\" gender male birthdate 18/10/1…\ntrue\n\n\n7292989\n\" work experience 10/15/2015 – …\ntrue\n\n\n7282933\n\" f o r m a t o e u r o p e o p…\ntrue\n\n\n7130948\n\" professional experience ener …\ntrue\n\n\n6871809\n\" experience bartender/barista …\ntrue\n\n\n\n\n\n\n\nprint(\"Sample CVs not in English:\")\nprint(not_english_df.sample(1)[\"Translated_CV\"].item())\n\nSample CVs not in English:\n timetables and maps of bus line 725 view on a web page 725 genova brignole bus line 725 (genova brignole) has 2 routes. during the week it is operational: (1) genova brignole: 05:04 - 20:04 (2) torriglia (→ scoffera): 06:30 - 20:30 use moovit to find the bus line 725 stops closest to you and find out when the next bus line 725 will pass in the direction of: genova brignole bus line 725 timetables departure times to genova brignole: 55 stops view the timetables for the line monday 05:04 - 20:04 tuesday 05:04 - 20:04 torriglia / rimessa wednesday 05:04 - 20:04 torriglia, piazza piaggio thursday 05:04 - 20:04 costa / laccetto friday 05:04 - 20:04 prato / rosciano saturday 05:04 - 20:04 laccio / strada nuova sunday 06:34 - 20:04 laccio / junction cavorsi - provincial depot passo della scoffera via scoffera / civ. 60 information on bus line 725 direction: genoa brignole via scoffera / civ. 38 stops: 55 journey time: 69 min sottocolle / from u bisaccia the line in brief: loc. sottocolle loc. fossa loc. piancarnasse loc. ferruccio loc. rapallin / former depot loc. rapallin bargagli / via martini 29 bargagli / carabinieri - s. alberto junction bargagli / pharmacy terminus bargagli center loc. rollavariante di traso trapena / florist via dei partigiani / f. 28 via dei partigiani / junction viganego loc. the capture via bavari / sherwood refuge crossroads for davagna via bavari / stoppani via bavari / house cantoniera prato / pian martello (terminus) struppa 1 / benedetto da porto struppa 3 / trossarelli struppa 5 / creto struppa 6 / doria struppa 7 / lucarno struppa 8 / ligorna struppa 10 / tour of fullo molassana 3 / gherzi molassana 4 / geirato molassana 5 / emilia piacenza 1 / rocca piacenza 3 / san gottardo piacenza 5 / bridge feritare piacenza 6 / san sebastiano piacenza 8 / bridge veronelli piacenza 11 / resasco piacenza 12 / cemetery staglieno bobbio 2 / montaldo bobbio 4 / bridge bobbio 5 / stadium canevari 1 / romagnosi canevari 2 / piazza firpo canevari 4/sant&#39;agata metrogenova brignole / zaninetta terminaldirection: torriglia (→ scoffera) timetables of bus line 725 departure times torriglia (→ scoffera): 54 stops view the timetables of the line monday 06:30 - 20:30 tuesday 06:30 - 20:30 genova brignole / zaninetta terminal wednesday 06:30 - 20:30 canevari/sant&#39;agata metro thursday 06:30 - 20:30 moresco 2/romagnosi friday 06:30 - 20:30 monnet/stadio saturday 06:30 - 20:30 bobbio 2/pontetto sunday 06:00 - 21:15 bobbio 4/montaldo piacenza 1/staglieno cemetery piacenza 5/ponte veronelli information on bus line 725 direction: torriglia (→ scoffera) piacenza 7/san sebastiano stops: 54 journey time: 65 min emilia 2/fossato cicala the line in brief: emilia 4/sciorba emilia 6/rocca emilia 7/molassana molassana 2/geirato molassana 3/gherzi struppa 3/ligorna struppa 4/lucarno struppa 5/doria struppa 7/rosata struppa 8/trossarelli struppa 10/suppini prato / pian martello (terminus) via bavari, 2 via bavari / loc. tane stoppani via bavari / junction for davagna via bavari / sherwood refuge loc. la presa via dei partigiani / viganego junction via dei partigianiloc. trapena florist loc. trapena via cevasco 17 / traso variant locality rolla bargagli / municipality (center) bargagli / pharmacy bargagli / carabinieri - s. alberto junction bargagli / via martini 64 loc. rapallin loc. rapallin / former garage loc. ferruccio loc. piancarnasse loc. fossa sottocolle / junction for chiavari sottocolle / from u bisaccia via scoffera / street number 43 via scoffera / street number 60 passo della scoffera laccio / junction cavorsi - provincial depot laccio / new road locality prato prato / rosciano costa / laccetto torriglia, piazza piaggio torriglia / garagetimetables, maps and stops of bus line 725 available in a pdf on moovitapp.com. use moovit app to get real wait times, schedules check arrival times on all other lines or step-by-step directions to get around by public transport in genoa and savona. real time discover moovit solutions maas available countries mooviter community © 2022 moovit - all rights reserved \n\n\nWe ran our check on the Translated_CV column and found only 16 records flagged as non-English. After a manual review, we discovered that these CVs are indeed written in English but include many Italian place names, addresses and organization titles which skew the language detector’s statistics. Since the underlying text is English and these cases are few, we will not drop them.\n\nNote: going forward, all regex or pattern-based quality checks should be applied only to the English Translated_CV field.\n\n\n\n6. Handling Empty CV Records\nDuring our inspection we found many corrupted CVs.  In each dedicated section, to ensure data quality and consistency across all our analyses, we will:\n\nExclude their parsed skills\nFilter out any rows in our parsed skills DataFrame (cv_skills) corresponding to those same CANDIDATE_IDs.\nOmit their entries in the Reverse Matching results\nDrop records in the Reverse Matching dataset (ReverseMatching.xlsx) for those candidate IDs.\n\nThis cleanup step prevents entirely missing CVs from biasing our skill‐extraction and matching analyses."
  },
  {
    "objectID": "notebooks/data_cleaning.html#parsed-skills-the-cv_skills-dataframe-that-feeds-the-matcher",
    "href": "notebooks/data_cleaning.html#parsed-skills-the-cv_skills-dataframe-that-feeds-the-matcher",
    "title": "Data Cleaning",
    "section": "2. Parsed Skills (the cv_skills DataFrame that feeds the matcher)",
    "text": "2. Parsed Skills (the cv_skills DataFrame that feeds the matcher)\n\n1. Schema & Load\n\nraw_cv = load_data(CANDIDATE_CVS_TRANSLATED_PATH)\nraw_skills = load_data(PARSED_DATA_PATH)\nprint(\"DataFrame schema:\", raw_skills.schema)\nprint(f\"Loaded: {raw_skills.height} Skills\")\n\n################\n\ntotal_rows = raw_skills.height\nunique_ids = raw_skills.select(pl.col(\"CANDIDATE_ID\")).unique().height\nprint(f\"Unique CANDIDATE_ID: {unique_ids}\")\n\nDataFrame schema: Schema({'CANDIDATE_ID': Int64, 'Skill': String, 'Skill_Type': String})\nLoaded: 132985 Skills\nUnique CANDIDATE_ID: 7769\n\n\nExclude parsed skills of the candidates with corrupted CVs.\n\nraw_cv_ids = raw_cv.select(pl.col(\"CANDIDATE_ID\")).to_series().to_list()\nprint(\"Number of CVs loaded -&gt;\", len(raw_cv_ids))\n\nraw_cv_cleaned_ids = raw_cv_cleaned.select(pl.col(\"CANDIDATE_ID\")).to_series().to_list()\nprint(\n    \"Number of CVs kept after cleaning -&gt;\",\n    len(raw_cv_cleaned_ids),\n)\n\nraw_cv_deleted_ids = (\n    raw_cv.filter(~pl.col(\"CANDIDATE_ID\").is_in(raw_cv_cleaned_ids))\n    .select(pl.col(\"CANDIDATE_ID\"))\n    .to_series()\n    .to_list()\n)\nprint(\n    \"Number of CVs deleted -&gt;\",\n    len(raw_cv_deleted_ids),\n)\n\nNumber of CVs loaded -&gt; 7769\nNumber of CVs kept after cleaning -&gt; 7521\nNumber of CVs deleted -&gt; 248\n\n\n\nraw_skills_cleaned = filter_out_candidate_ids(\n    raw_skills, raw_cv_deleted_ids, df_name=\"Skills\", description=\"CVs deleted\"\n)\n\n##################\n\n# Sanity check on the new number of unique candidate IDs.\n\ntotal_rows = raw_skills_cleaned.height\nunique_ids = raw_skills_cleaned.select(pl.col(\"CANDIDATE_ID\")).unique().height\nprint(f\"Unique CANDIDATE_ID: {unique_ids}\")\n\nSkills with CVs deleted: 4086 out of 132985 total\nThat is 3.07% of all skills\nSkills -&gt; Original rows: 132985\nSkills -&gt; Dropped rows:  4086 (IDs in provided list)\nSkills -&gt; Remaining rows: 128899\nUnique CANDIDATE_ID: 7521\n\n\n\n\n2. Missing Value Inspection\n\nmissing_skills = inspect_missing(raw_skills_cleaned)\n\nMissing value summary:\n\n\n\n\nshape: (3, 3)\n\n\n\ncolumn\nn_missing\npct_missing\n\n\nstr\nu32\nf64\n\n\n\n\n\"CANDIDATE_ID\"\n0\n0.0\n\n\n\"Skill\"\n82\n0.06\n\n\n\"Skill_Type\"\n0\n0.0\n\n\n\n\n\n\n\nColumns with &gt; 0% missing values:\n\n\n\n\nshape: (1, 3)\n\n\n\ncolumn\nn_missing\npct_missing\n\n\nstr\nu32\nf64\n\n\n\n\n\"Skill\"\n82\n0.06\n\n\n\n\n\n\nInspecting missing values we’ve found that 82 rows are missing a Skill value. Diving into the data, we see that these rows are all associated with the value DRIVERSLIC in the Skill_Type column.  This indicates that these rows are likely placeholders for driver license information, which is however useful to our analysis.  We will not drop these rows from the dataset.\nRemove Missing or Null Skills\n\ninvalid_rows = raw_skills_cleaned.filter(\n    pl.col(\"Skill\").is_null() & (pl.col(\"Skill_Type\") != \"DRIVERSLIC\")\n)\nprint(f\"Found {invalid_rows.height} rows with missing Skill (excluding DRIVERSLIC):\")\ndisplay(invalid_rows)\n\n##################\n\nraw_skills_cleaned = raw_skills_cleaned.filter(\n    ~(pl.col(\"Skill\").is_null() & (pl.col(\"Skill_Type\") != \"DRIVERSLIC\"))\n)\nprint(f\"After dropping invalid rows, {raw_skills_cleaned.height} skill records remain.\")\n\nFound 1 rows with missing Skill (excluding DRIVERSLIC):\n\n\n\nshape: (1, 3)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\n\n\ni64\nstr\nstr\n\n\n\n\n5107653\nnull\n\"Language_Skill\"\n\n\n\n\n\n\nAfter dropping invalid rows, 128898 skill records remain.\n\n\n\n\n3. Find exact duplicate\n\ntotal_before = raw_skills_cleaned.height\nunique_before = raw_skills_cleaned.unique(subset=[\"CANDIDATE_ID\", \"Skill\"]).height\n\nprint(f\"Total rows: {total_before}\")\nprint(f\"Unique (ID, Skill, Skill_Type): {unique_before}\")\n\n##################\n\nif total_before != unique_before:\n    raw_skills_cleaned = raw_skills_cleaned.unique(subset=[\"CANDIDATE_ID\", \"Skill\"])\n    print(f\"Dropped {total_before - unique_before} duplicates.\")\nelse:\n    print(\"No duplicates found—skipping deduplication.\")\n\nTotal rows: 128898\nUnique (ID, Skill, Skill_Type): 128898\nNo duplicates found—skipping deduplication.\n\n\n\n\n4. Validate Skill Types\n\nEnsure Skill_Type only takes one of your known categories (IT_Skill, Job_title, Language_Skill, Professional_Skill, DRIVERSLIC).\n\n\nVALID_TYPES = [\n    \"IT_Skill\",\n    \"Job_title\",\n    \"Language_Skill\",\n    \"Professional_Skill\",\n    \"DRIVERSLIC\",\n]\n\nall_types = raw_skills_cleaned.select(\"Skill_Type\").unique().to_series().to_list()\ninvalid_types = [t for t in all_types if t not in VALID_TYPES]\n\n##############\n\nif invalid_types:\n    print(\"Found invalid Skill_Type values:\")\n    for t in invalid_types:\n        print(\"  -\", t)\nelse:\n    print(\"All Skill_Type values are valid.\")\n\nAll Skill_Type values are valid.\n\n\n\n\n5. Filter Out Garbage Skill Tokens\n\nDrop any Skill entries that are obviously malformed (e.g. length &lt; 2 or &gt; 100, only punctuation or digits).\n\nOptionally remove entries matching placeholder patterns (like a long run of X).\n\n\ndropped_skills = find_dropped_skill_rows(raw_skills)\nprint(f\"Dropped {dropped_skills.height} skill rows:\")\ndisplay(dropped_skills.head(10))\n\nDropped 88 skill rows:\n\n\n\nshape: (10, 3)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\n\n\ni64\nstr\nstr\n\n\n\n\n7747553\nnull\n\"DRIVERSLIC\"\n\n\n7583771\nnull\n\"DRIVERSLIC\"\n\n\n7527802\nnull\n\"DRIVERSLIC\"\n\n\n7484890\nnull\n\"DRIVERSLIC\"\n\n\n7456557\nnull\n\"DRIVERSLIC\"\n\n\n7422185\nnull\n\"DRIVERSLIC\"\n\n\n7413665\nnull\n\"DRIVERSLIC\"\n\n\n7347469\nnull\n\"DRIVERSLIC\"\n\n\n7164548\nnull\n\"DRIVERSLIC\"\n\n\n7099403\nnull\n\"DRIVERSLIC\"\n\n\n\n\n\n\n\ndisplay(dropped_skills.filter(pl.col(\"Skill_Type\") == \"DRIVERSLIC\"))\n\n\nshape: (87, 3)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\n\n\ni64\nstr\nstr\n\n\n\n\n7747553\nnull\n\"DRIVERSLIC\"\n\n\n7583771\nnull\n\"DRIVERSLIC\"\n\n\n7527802\nnull\n\"DRIVERSLIC\"\n\n\n7484890\nnull\n\"DRIVERSLIC\"\n\n\n7456557\nnull\n\"DRIVERSLIC\"\n\n\n…\n…\n…\n\n\n124012\nnull\n\"DRIVERSLIC\"\n\n\n123094\nnull\n\"DRIVERSLIC\"\n\n\n117943\nnull\n\"DRIVERSLIC\"\n\n\n117855\nnull\n\"DRIVERSLIC\"\n\n\n42289\nnull\n\"DRIVERSLIC\"\n\n\n\n\n\n\nAll the dropped_skills would be DRIVERSLIC, so we will not drop any rows from the skills, as previously discussed."
  },
  {
    "objectID": "notebooks/data_cleaning.html#matcher-results-output-csv-from-the-matching-algorithm",
    "href": "notebooks/data_cleaning.html#matcher-results-output-csv-from-the-matching-algorithm",
    "title": "Data Cleaning",
    "section": "3. Matcher Results (output CSV from the matching algorithm)",
    "text": "3. Matcher Results (output CSV from the matching algorithm)\n\nreversed_skills_matching_dict = load_excel_sheets(\n    REVERSE_MATCHING_PATH, sheets=[\"Candidates\"]\n)\nreversed_skills_matching = reversed_skills_matching_dict[\"Candidates\"]\n\nprint(\"DataFrame schema:\", reversed_skills_matching.schema)\nprint(f\"Loaded: {reversed_skills_matching.height} candidates\")\n\n################\n\ntotal_rows = reversed_skills_matching.height\nunique_ids = reversed_skills_matching.select(pl.col(\"CANDIDATE_ID\")).unique().height\nprint(f\"Unique CANDIDATE_ID: {unique_ids}\")\n\nDataFrame schema: Schema({'CANDIDATE_ID': Int64, 'Gender': String, 'Age_bucket': String, 'professional_categories_int': String, 'matterknowledges': String, 'languages': String, 'regulatedtrainings': String, 'candidatecity': String, 'drivinglicenses': String, 'LONGITUDE': Float64, 'LATITUDE': Float64, 'experience': String})\nLoaded: 7769 candidates\nUnique CANDIDATE_ID: 7769\n\n\n\nreversed_skills_matching = filter_out_candidate_ids(\n    reversed_skills_matching,\n    raw_cv_deleted_ids,\n    df_name=\"Reverse Matchhing Candidates\",\n    description=\"CVs deleted\",\n)\n\n##################\n\n# Sanity check on the new number of unique candidate IDs.\n\ntotal_rows = reversed_skills_matching.height\nunique_ids = reversed_skills_matching.select(pl.col(\"CANDIDATE_ID\")).unique().height\nprint(f\"Unique CANDIDATE_ID: {unique_ids}\")\n\nReverse Matchhing Candidates with CVs deleted: 248 out of 7769 total\nThat is 3.19% of all reverse matchhing candidates\nReverse Matchhing Candidates -&gt; Original rows: 7769\nReverse Matchhing Candidates -&gt; Dropped rows:  248 (IDs in provided list)\nReverse Matchhing Candidates -&gt; Remaining rows: 7521\nUnique CANDIDATE_ID: 7521\n\n\n\nmissing_reversed_skills = inspect_missing(reversed_skills_matching)\n\nMissing value summary:\n\n\n\n\nshape: (12, 3)\n\n\n\ncolumn\nn_missing\npct_missing\n\n\nstr\nu32\nf64\n\n\n\n\n\"CANDIDATE_ID\"\n0\n0.0\n\n\n\"Gender\"\n0\n0.0\n\n\n\"Age_bucket\"\n0\n0.0\n\n\n\"professional_categories_int\"\n7\n0.09\n\n\n\"matterknowledges\"\n3818\n50.76\n\n\n…\n…\n…\n\n\n\"candidatecity\"\n0\n0.0\n\n\n\"drivinglicenses\"\n3586\n47.68\n\n\n\"LONGITUDE\"\n0\n0.0\n\n\n\"LATITUDE\"\n0\n0.0\n\n\n\"experience\"\n2\n0.03\n\n\n\n\n\n\n\nColumns with &gt; 0% missing values:\n\n\n\n\nshape: (6, 3)\n\n\n\ncolumn\nn_missing\npct_missing\n\n\nstr\nu32\nf64\n\n\n\n\n\"professional_categories_int\"\n7\n0.09\n\n\n\"matterknowledges\"\n3818\n50.76\n\n\n\"languages\"\n4011\n53.33\n\n\n\"regulatedtrainings\"\n4143\n55.09\n\n\n\"drivinglicenses\"\n3586\n47.68\n\n\n\"experience\"\n2\n0.03\n\n\n\n\n\n\n\nAnalyze demographic data\nNext, we examine the demographic columns (age, gender, and location) to spot correlations and identify any redundant fields.\n\nreversed_skills_matching = reversed_skills_matching.with_columns(\n    pl.when(pl.col(\"LATITUDE\") &gt; 44.5)\n    .then(pl.lit(\"NORTH\"))\n    .when(pl.col(\"LATITUDE\") &lt; 42)\n    .then(pl.lit(\"SOUTH\"))\n    .otherwise(pl.lit(\"CENTER\"))\n    .alias(\"Location\")\n)\n\nAfter classifying candidates as North, Center, or South by latitude, we select the columns of interest and examine their inter-correlations to decide which ones to analyze next.\n\nreversed_skills_matching_filtered = filter_unknown_and_other_rows(\n    reversed_skills_matching\n)\nreversed_skills_matching_filtered = reversed_skills_matching_filtered.select(\n    \"Gender\",\n    \"Age_bucket\",\n    \"Location\",\n)\nreversed_skills_matching_filtered = reversed_skills_matching_filtered.drop_nulls()\ndisplay(reversed_skills_matching_filtered)\n\n\nshape: (7_404, 3)\n\n\n\nGender\nAge_bucket\nLocation\n\n\nstr\nstr\nstr\n\n\n\n\n\"Female\"\n\"55-74\"\n\"NORTH\"\n\n\n\"Female\"\n\"55-74\"\n\"CENTER\"\n\n\n\"Female\"\n\"55-74\"\n\"NORTH\"\n\n\n\"Male\"\n\"25-34\"\n\"NORTH\"\n\n\n\"Female\"\n\"55-74\"\n\"CENTER\"\n\n\n…\n…\n…\n\n\n\"Female\"\n\"55-74\"\n\"NORTH\"\n\n\n\"Male\"\n\"25-34\"\n\"CENTER\"\n\n\n\"Male\"\n\"25-34\"\n\"SOUTH\"\n\n\n\"Male\"\n\"25-34\"\n\"NORTH\"\n\n\n\"Female\"\n\"55-74\"\n\"NORTH\"\n\n\n\n\n\n\nBuilding a Cramér’s V Association Matrix\nTo find inter correlations, we compute a square matrix whose (i, j) entry is the Cramér’s V between column i and column j in reversed_skills_matching. (computed with polars-ds)\nIt normalises the χ² test of independence to the range [0, 1]:\n\\[\nV \\;=\\; \\sqrt{\\frac{\\chi^{2}}{\\,n\\bigl(k_{\\min}-1\\bigr)}}\n\\]\nwhere\n\nχ² is the chi-square statistic,\nn is the number of rows,\n\\(k_{\\min}\\) is the smaller of the two variables’ unique value counts.\n\nThus V = 0 means no association, V = 1 means a perfect link.\n\ncramer_corrs = [\n    [\n        cramers_v(reversed_skills_matching_filtered, c1, c2)\n        for c2 in reversed_skills_matching_filtered.columns\n    ]\n    for c1 in reversed_skills_matching_filtered.columns\n]\n\ncramer_matrix = (\n    pl.DataFrame(cramer_corrs, schema=reversed_skills_matching_filtered.columns)\n    .with_columns(pl.Series(\"index\", reversed_skills_matching_filtered.columns))\n    .unpivot(index=\"index\")\n    .pivot(index=\"index\", on=\"variable\", values=\"value\")\n    .sort(\"index\")\n)\n\n\nAttention!\nThe heat-map reveals a striking finding: Age Bucket and Gender are perfectly correlated (Cramér’s V = 1.0).\nSince either variable fully determines the other, we will keep only one in subsequent analyses to avoid redundancy.\n\n\nplot_cramer_matrix(cramer_matrix)\n\n\n\n\n\n\n\n\nThe tables below makes this explicit: every female candidate is aged 55 – 74, while every male candidate is aged 25 – 34.\n\nfemale_candidates = reversed_skills_matching_filtered.filter(\n    pl.col(\"Gender\") == \"Female\"\n)\ndisplay(female_candidates[\"Age_bucket\"].value_counts())\n\n\nshape: (1, 2)\n\n\n\nAge_bucket\ncount\n\n\nstr\nu32\n\n\n\n\n\"55-74\"\n3420\n\n\n\n\n\n\n\nmale_candidates = reversed_skills_matching_filtered.filter(pl.col(\"Gender\") == \"Male\")\ndisplay(male_candidates[\"Age_bucket\"].value_counts())\n\n\nshape: (1, 2)\n\n\n\nAge_bucket\ncount\n\n\nstr\nu32\n\n\n\n\n\"25-34\"\n3984"
  },
  {
    "objectID": "notebooks/data_cleaning.html#exporting-the-cleaned-data",
    "href": "notebooks/data_cleaning.html#exporting-the-cleaned-data",
    "title": "Data Cleaning",
    "section": "Exporting the Cleaned Data",
    "text": "Exporting the Cleaned Data\n\nExport the cleaned CVs, the cleaned parsed skills, the cleaned matcher result to a new CSV file, ensuring the schema and data types are preserved.\n\n\nraw_cv_cleaned.write_csv(\n    \"../data/Adecco_Dataset_cleaned/CV_translated_cleaned.csv\", separator=\";\"\n)\nraw_skills_cleaned.write_csv(\n    \"../data/Adecco_Dataset_cleaned/Skills_cleaned.csv\", separator=\";\"\n)\nreversed_skills_matching.write_csv(\n    \"../data/Adecco_Dataset_cleaned/reversed_skills_matching_candidate.csv\",\n    separator=\";\",\n)"
  },
  {
    "objectID": "pages/03-results.html",
    "href": "pages/03-results.html",
    "title": "Results",
    "section": "",
    "text": "In this section we walk through our key findings:"
  },
  {
    "objectID": "pages/03-results.html#error-detection",
    "href": "pages/03-results.html#error-detection",
    "title": "Results",
    "section": "1. Error detection",
    "text": "1. Error detection\nAs previously explained on the Pipeline page, our study first tried to highlight the errors the parser makes in categories such as Language, Driving License, and Job Title.\nWhile for the Driving License we treat the task as a binary has_any_driving_license label for each candidate, for the other two categories we use a finer granularity, examining the agreement between our method and the parser on every individual skill.\n\nResults\n\n\n\n\n\n\n\n\n\n\n\n\nSkill\nTP\nFP\nTN\nFN.\nPrecision\nRecall\n\n\n\n\nDriving License\n1689\n643\n2927\n2445\n0.83\n0.41\n\n\nLanguage skills\n10389\n2392\n132\n5438\n0.81\n0.66\n\n\nJob titles\n7838\n14976\n326\n6394\n0.34\n0.551\n\n\n\n\n\nDemo\n\nHere the demo of our bias-detection dashboard, where you can filter by gender, language or job titles and immediately see extraction disagreement:"
  },
  {
    "objectID": "pages/03-results.html#key-bias-metrics",
    "href": "pages/03-results.html#key-bias-metrics",
    "title": "Results",
    "section": "2. Key Bias Metrics",
    "text": "2. Key Bias Metrics\nDue to the presence of numerous errors from the parser, we focused our analysis on the previously discussed demographic groups. To do so, we employed several metrics that specifically account for false negatives, since these result from exact matches and therefore offer high reliability.\nTo assess the model’s fairness, we employed the following bias detection metrics:\n\n\n\n\n\n\n\n\nMetric\nFormula\nInterpretation\n\n\n\n\nEquality of Opportunity (TPR parity)\n\\[\\text{TPR}_g = \\frac{TP_g}{TP_g + FN_g} \\]\n\\(\\text{TPR}_g\\) equal for every \\(g\\) ensures that every individual who truly qualifies for a positive outcome has the same chance of being correctly identified, regardless of group membership.\n\n\n Calibration (NPV)\n\\[\\text{NPV}_g = \\frac{TN_g}{TN_g + FN_g}\\qquad \\]\n\\(\\text{NPV}_g\\) parity for every \\(g\\) ensures that when the model predicts a negative outcome, the probability of being correct is the same for every group.\n\n\n Selection Rate\n\\[\\text{SR}_g = \\frac{TP_g + FP_g}{TP_g + FP_g + TN_g + FN_g} \\]\n Share of individuals in group \\(g\\) predicted positive (selected).\n\n\n Disparate Impact (DI)\n\\[\\displaystyle DI = \\frac{\\text{SR}_{\\text{target}}}{\\text{SR}_{\\text{reference}}}\\]\nRatio of selection rates; values &lt; 0.80 (four-fifths rule) indicate potential adverse impact against the target group.\n\n\n\nAll these metrics were computed for all the groups to detect and quantify possible bias in the selection process.\n\nDriving License:\n\n\n\n\n\n\n\n\n\nGender\nTPR\nNPV\nDI\n\n\n\n\nMale\n0.42\n0.53\n1.00\n\n\nFemale\n0.39\n0.56\n0.88\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nTPR\nNPV\nDI\n\n\n\n\nNorth\n0.40\n0.54\n1.00\n\n\nCenter\n0.43\n0.55\n1.08\n\n\nSouth\n0.40\n0.56\n0.97\n\n\n\n\n\n\n\n\n\n\n\n\nLenght\nTPR\nNPV\nDI\n\n\n\n\nLong\n0.38\n0.46\n1.00\n\n\nMedium\n0.44\n0.60\n0.99\n\n\nShort\n0.47\n0.79\n0.75\n\n\n\n\n\nLanguage skills:\n\n\n\n\n\n\n\n\n\nGender\nTPR\nNPV\nDI\n\n\n\n\nMale\n0.67\n0.03\n1.00\n\n\nFemale\n0.64\n0.02\n0.95\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nTPR\nNPV\nDI\n\n\n\n\nNorth\n0.66\n0.03\n1.00\n\n\nCenter\n0.66\n0.01\n1.01\n\n\nSouth\n0.65\n0.02\n0.99\n\n\n\n\n\n\n\n\n\n\n\n\nLenght\nTPR\nNPV\nDI\n\n\n\n\nLong\n0.63\n0.01\n1.00\n\n\nMedium\n0.69\n0.03\n1.11\n\n\nShort\n0.69\n0.11\n1.19\n\n\n\n\n\nJob titles:\n\n\n\n\n\n\n\n\n\nGender\nTPR\nNPV\nDI\n\n\n\n\nMale\n0.53\n0.06\n1.00\n\n\nFemale\n0.56\n0.03\n0.97\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\nTPR\nNPV\nDI\n\n\n\n\nNorth\n0.55\n0.05\n1.00\n\n\nCenter\n0.56\n0.04\n0.99\n\n\nSouth\n0.53\n0.06\n0.97\n\n\n\n\n\n\n\n\n\n\n\n\nLenght\nTPR\nNPV\nDI\n\n\n\n\nLong\n0.56\n0.01\n1.00\n\n\nMedium\n0.53\n0.07\n0.99\n\n\nShort\n0.51\n0.28\n0.97"
  },
  {
    "objectID": "pages/03-results.html#summary-of-findings",
    "href": "pages/03-results.html#summary-of-findings",
    "title": "Results",
    "section": "3. Summary of Findings",
    "text": "3. Summary of Findings\nOverall, the parser exhibits very high error rates, with low recall across all categories and a large number of false negatives, indicating that many true skills are missed by the system.\nWhen we examine the metrics across demographic groups, no strong bias emerges; nonetheless, a few observations merit discussion:\n\nMinimal gender disparity in Driving License: the DI for females is 0.88, slightly below that of males but still above the critical 0.80 threshold defined by the four-fifths rule.\nLength-based imbalances: we observe that “Short” CVs are disadvantaged in driving-license extraction (DI = 0.75) and simultaneously advantaged for language skill extraction (DI = 1.19). These opposite effects suggest the parser’s performance varies significantly with document length and deserve a more thorough analysis to uncover the root causes of these imbalances and guide mitigation strategies."
  },
  {
    "objectID": "pages/02-pipeline.html",
    "href": "pages/02-pipeline.html",
    "title": "Pipeline",
    "section": "",
    "text": "Here we outline the work steps we followed to answer the questions in our problem statement. In the first subsection, we cover two initial setup tasks that prepare the data for the rest of the pipeline:\n\nTranslating CVs from Italian into English so we can apply semantic-matching techniques consistently.\nLabeling with an LLM each parser extracted skill as either a hard skill or a soft skill.  🔗 See the Hard Soft skills labelling notebook\n\nWith these preparations in place, our pipeline we built is divided into the following steps:\n\nData Cleaning\nWe standardized and filtered the raw, anonymized CV texts with structural issues as:\n\nEmpty / Whitespace-Only/ Very Short Text\nHigh Repetition / Low Structure\nLow token & vocabulary Richness\nCorrupted CVs with a fraction of invalid characters\nCVs with Placeholder Tails (e.g. “XXXXXXXXXX…”)\nCVs with Poor Translations\n\nParsed skills of the candidates with corrupted CVs has been also excluded.\nThe candidates’ demographic data (age, gender, and geographic location) were then analyzed to uncover any potential correlations or redundancies among the features.\n🔗 See the Cleaning notebook\nExploratory Analysis\nOnce the data had been cleaned, we conducted an exploratory analysis of the skills extracted by the parser, focusing on:\n\nDistribution of parsed skills and job information : explore how different skills are distributed across roles and sectors.\nSkills per candidate distribution: analyze the number of skills each candidate has, flagging any outliers.\nHard vs. soft skills: examine and compare the distributions of hard skills versus soft skills.\n\n\n🔗 See the Exploration notebook\nDistributions Analysis\nThis analysis aims to uncover distributional imbalances in the parsed skills by considering the following demographic data:\n\nGender\nLocation\nHard vs. soft skills\n\nTo quantify these imbalances, we have defined disparity metrics such as the Gini index and a custom bias strength measure.\n\nWhy This Matters:  Detecting these imbalances is critical to designing a robust, fair pipeline that flags biases introduced by the CV parser relying only on raw CV inputs and their parsed outputs.\n\n\n🔗 See the Distributions Analysis notebook\nBias Detection\nAfter analyzing the various imbalances in the distributions of skills extracted by the parser, we tackled bias detection through a hybrid pipeline:\n\nOn the skills side (specifically DRIVERSLIC and LANGUAGE_SKILL), we implemented a regex-based extractor that systematically scans each CV for patterns associated with driving licenses and language proficiencies, ensuring we capture every variant and format used by candidates.\nOn the job-title side, we deployed a two-step process:\n\nRule-based: exact matching against the official ESCO european job list (link), pulling in any titles that appear in the raw CV text.\nSemantic matching: catches synonyms or paraphrases of the same job experience, making our extraction robust to differences in how the parser names each job.  eg. ESCO: Postman, Parsed skill: Postal operator, Similarity: 0.64\n\n\nUsing our pipeline results as the ground truth, we measured the overall performance of the proprietary parser by comparing its outputs against our method and analyzing their agreement.\nWe focused on false negatives: candidates for whom our extractor found a match but the parser returned nothing.\nTo make these insights more accessible, we built a Streamlit app that lets you dive into individual candidates. For each false negative, the interface on the left displays the raw CV text with regex matched segments highlighted in red and on the right shows the skills the parser actually assigned to that candidate.\nOnce we reviewed and analyzed the results of our methods, we broadened our bias assessment to the three groups previously identified, (gender, geographical position, hard vs soft skills) to determine whether the parser’s errors disproportionately impact under or over represented populations.\n🔗 See the Bias Detection notebook"
  },
  {
    "objectID": "hiring_cv_bias/hard_soft_skills_labelling/hard_soft_skill_labelling.html",
    "href": "hiring_cv_bias/hard_soft_skills_labelling/hard_soft_skill_labelling.html",
    "title": "Hard Soft skills labelling",
    "section": "",
    "text": "Main Objective:\nThis notebook automates the labeling of extracted Professional_Skill entries as Hard, Soft, or Unknown.\nWe will do this using a 4-bit quantized Llama-3.1 8B Instruct model on a GPU P100.\nSteps: 1. It loads the list of unique skills 2. Classifies each skill in batches via the language model 3. Evaluate the model’s accuracy using the technique LLM as a judge via Chat Gpt o3 model. 4. Writes the results to a CSV.\nAdditionally, we include the overall breakdown, the model’s Chain of Thought and each skill’s individual classification in the CoT.log file.\n\nimport numpy as np\nimport polars as pl\nimport torch\nfrom huggingface_hub import login\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nfrom hiring_cv_bias.config import CLEANED_SKILLS, HARD_SOFT_SKILLS\nfrom hiring_cv_bias.hard_soft_skills_labelling.utils import (\n    batch_classify_skills,\n    clean_results,\n)\nfrom hiring_cv_bias.utils import load_data\n\nSEED = 42\nlogin(token=\"[YOUR_TOKEN]\")\n\n\ncv_skills = load_data(CLEANED_SKILLS)\nskills = (\n    cv_skills.filter(pl.col(\"Skill_Type\") == \"Professional_Skill\")[\"Skill\"]\n    .unique()\n    .to_list()\n)\n\nHere we loads the model (using nf4 and float16) and its tokenizer.\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n)\n\nThe function batch_classify_skills in utils.py does the work. Let’s breakdown it:\nInputs:\n- model: A HuggingFace causal LM model instance (e.g. quantized Llama-3.1-8B Instruct).\n- tokenizer: Corresponding tokenizer for the model.\n- skills: List of skill strings to classify.\n- batch_size: Number of skills to send to the model at once.\nProcess:\n1. Iterate over the skills list in chunks of size batch_size.\n2. For each batch, construct a prompting template that:\n- Instructs the model to think step by step about the skill.\n- Provides four concrete examples (Data Analysis -&gt; Hard; Communication -&gt; Soft).\n\nTokenize all prompts simultaneously with padding/truncation and move tensors to the model’s device.\n\nCall model.generate(...) to produce completions (up to 150 new tokens) for each prompt.\n\nDecode each generated output, extract the final token as the predicted label (Hard, Soft, or Unknown) and append to labels.\n\nReturn the full list of labels in the same order as the input skills.\n\n\nhard_soft_labels = batch_classify_skills(skills, batch_size=64)\n\n\n(\n    hard_soft_labels.count(\"Hard\"),\n    hard_soft_labels.count(\"Soft\"),\n    hard_soft_labels.count(\"Unknown\"),\n)\n\n\noutput_df = pl.DataFrame({\"Skill\": skills, \"label\": hard_soft_labels})\noutput_df.write_csv(\"hard_soft_skills.csv\")\n\n\nEstimating Accuracy\nWe estimated the model’s accuracy using a sample of 100 skills: 50 predicted as hard skills and 50 predicted as soft skills.\nWe then compared the model’s predictions with evaluations provided by ChatGPT o3 model.\nFrom this comparison, we derived separate accuracy estimates for hard and soft skills, as well as an overall accuracy score.\nAccuracy: - Hard –&gt; 49/50 - Soft –&gt; 23/50 - Overall –&gt; 72/100\n\nhard_soft_df = load_data(HARD_SOFT_SKILLS)\nprint(\n    hard_soft_df[\"label\"]\n    .value_counts()\n    .filter(pl.col(\"label\").is_in([\"Hard\", \"Soft\", \"Unknown\"]))\n    .sort(pl.col(\"count\"), descending=True)\n)\n\nNow any label not equal to Hard, Soft or Unknown is replaced with Unknown. This step helps correct misclassifications arising from the model’s reasoning (e.g. truncated responses or unexpected formats) since we use the last token of its output as the predicted label.\n\nhard_soft_df = clean_results(hard_soft_df)\nprint(\n    hard_soft_df[\"label\"]\n    .value_counts()\n    .filter(pl.col(\"label\").is_in([\"Hard\", \"Soft\", \"Unknown\"]))\n    .sort(pl.col(\"count\"), descending=True)\n)\n\nAccuracy: - Hard –&gt; 46/50 - Soft –&gt; 16/50 - Overall –&gt; /100\n\nhard_skills = (\n    hard_soft_df.filter(pl.col(\"label\") == \"Hard\")\n    .sample(50, shuffle=True, seed=SEED)\n    .to_numpy()\n)\nsoft_skills = (\n    hard_soft_df.filter(pl.col(\"label\") == \"Soft\")\n    .sample(50, shuffle=True, seed=SEED)\n    .to_numpy()\n)\n\nskills_sample = np.concatenate((hard_skills, soft_skills))\nnp.random.shuffle(skills_sample)\nskills_sample"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bias Analysis in CV Parsing Pipeline",
    "section": "",
    "text": "LLMs can be subject to cultural biases. One such domain is recruiting, where LLMs are used to extract information from candidates’ CVs and then to select the most suitable profiles. These biases may favour or penalize applicants based on language or cultural cues, risking the exclusion of qualified talent."
  },
  {
    "objectID": "index.html#repository-structure",
    "href": "index.html#repository-structure",
    "title": "Bias Analysis in CV Parsing Pipeline",
    "section": "Repository Structure",
    "text": "Repository Structure\n\n── hiring_cv_bias\n│   ├── bias_detection\n│   │   ├── fuzzy\n│   │   │   ├── matcher.py  # perfoms matching between our extracted and parser skills \n│   │   │   ├── parser.py  # performs exact matching on CVs with a list of jobs titles \n│   │   │   └── utils.py # job filtering  \n│   │   └── rule_based\n│   │       ├── app\n│   │       │   └── fn_app.py # visualization app for extraction pipeline \n│   │       ├── evaluation\n│   │       │   ├── compare_parser.py # computes bias detection metrics for each group \n│   │       │   └── metrics.py  \n│   │       ├── extractors.py # extract and apply regex patterns \n│   │       ├── patterns.py # define patterns for exact matching \n│   │       └── utils.py\n│   ├── cleaning\n│   │   ├── common.py \n│   │   └── raw_cv.py # cleaning of corrupted CVs \n│   ├── config.py\n│   ├── exploration\n│   │   ├── gender_analysis.py # computes bias_strenght metric for each skill type   \n│   │   ├── disparity.py # computes Gini-Index metric for each skill type \n│   │   ├── visualize.py # plotting functions for visualizing distribution\n│   │   └── utils.py \n│   ├── hard_soft_skills_labelling\n│   │   ├── CoT.log # full CoT of the model used \n│   │   ├── hard_soft_skill_labelling.ipynb \n│   │   └── utils.py # automates the labeling of extracted Professional_Skill entries \n│   ├── translation\n│   │   └── translate.py # script for translating CVs in English \n│   └── utils.py\n│\n├── notebooks\n│   ├── data_cleaning.ipynb  \n│   ├── data_exploration.ipynb\n│   ├── distributions_analysis.ipynb\n│   └── bias_detection.ipynb"
  },
  {
    "objectID": "pages/01-problem.html",
    "href": "pages/01-problem.html",
    "title": "Problem Statement",
    "section": "",
    "text": "The goal of this study is to evaluate the potential biases in a proprietary CV parser, which automatically extracts skills from anonymized, raw resumes.\nSpecifically, we aim to answer:\n\nCandidate Representation:\n\n\nHow are candidates’ profiles represented in the parsing outputs?\nAre there systematic differences in the number or types of skills extracted across CVs?\n\n\nGendered Skill Association:\n\n\nDo certain skills appear significantly more often in profiles inferred to be male versus female?\nCould these patterns reflect underlying parser assumptions or training data imbalances?\n\n\nCultural and Geographical Bias:\n\n\nAre skill categories ( skill_type) that align with specific cultural or regional backgrounds overrepresented?\n\n\nHard vs. Soft Skills Distribution:\n\n\nWhat is the relative frequency of hard skills versus soft skills in the parsed output?\nDoes the parser systematically under‐detect one category, potentially skewing candidate profiles?\n\n\nDemographic Underrepresentation:\n\n\nAre certain demographic groups (e.g., inferred by language, region, or other proxies) underrepresented in the overall skill set?"
  },
  {
    "objectID": "notebooks/data_exploration.html",
    "href": "notebooks/data_exploration.html",
    "title": "Data exploration",
    "section": "",
    "text": "Main objective\nIn this notebook, the main objectives are: * Explore the distribution of parsed skills and job information. * Analyze the distribution of the number of skills per candidate and identify outliers. * Examine the distributions of hard and soft skills.\n\n%load_ext autoreload \n%autoreload 2\nimport matplotlib.pyplot as plt\nimport polars as pl\n\nfrom hiring_cv_bias.config import (\n    CLEANED_SKILLS,\n    HARD_SOFT_SKILLS,\n)\nfrom hiring_cv_bias.exploration.utils import plot_boxplot, plot_distribution_bar\nfrom hiring_cv_bias.exploration.visualize import (\n    plot_skills_frequency,\n    plot_skills_per_category,\n    plot_top_skills_for_job_title,\n)\nfrom hiring_cv_bias.utils import load_data\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ncv_skills = load_data(CLEANED_SKILLS)\ncv_skills.head(10)\n\n\nshape: (10, 3)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\n\n\ni64\nstr\nstr\n\n\n\n\n7990324\n\"Computer Literacy\"\n\"IT_Skill\"\n\n\n7990324\n\"Dental Assistant (m/f)\"\n\"Job_title\"\n\n\n7990324\n\"General Labourer (other) (m/f)\"\n\"Job_title\"\n\n\n7990324\n\"Intern (m/f)\"\n\"Job_title\"\n\n\n7990324\n\"Italian\"\n\"Language_Skill\"\n\n\n7974050\n\"Adobe Dreamweaver\"\n\"IT_Skill\"\n\n\n7974050\n\"ECDL\"\n\"IT_Skill\"\n\n\n7974050\n\"HTML\"\n\"IT_Skill\"\n\n\n7974050\n\"Microsoft Access\"\n\"IT_Skill\"\n\n\n7974050\n\"Microsoft Office\"\n\"IT_Skill\"\n\n\n\n\n\n\n\nSkill Extraction by Category\nThe bar chart displays the total number of skills extracted for each skill_type:\n\nProfessional_Skill: ~68,000 occurrences — the most frequently identified category.\n\nJob_title: ~23,000 occurrences.\n\nIT_Skill: ~22,000 occurrences.\n\nLanguage_Skill: ~13,000 occurrences.\n\nDRIVERSLIC: ~2,500 occurrences — the least common category.\n\n\nskill_counts = (\n    cv_skills.group_by(\"Skill_Type\")\n    .agg(pl.count(\"Skill_Type\").alias(\"count\"))\n    .sort(\"count\", descending=True)\n)\ndisplay(skill_counts)\n\n\nshape: (5, 2)\n\n\n\nSkill_Type\ncount\n\n\nstr\nu32\n\n\n\n\n\"Professional_Skill\"\n67873\n\n\n\"Job_title\"\n23169\n\n\n\"IT_Skill\"\n22415\n\n\n\"Language_Skill\"\n13075\n\n\n\"DRIVERSLIC\"\n2366\n\n\n\n\n\n\n\nplot_skills_frequency(cv_skills)\n\n\n\n\n\n\n\n\n\n\nTop N Skills by Category\nThe plot_skills_per_category function can be used to visualize the most frequent skills within any given skill category. It:\n\nFilters the cv_skills DataFrame by the chosen skill_type.\n\nComputes the frequency of each individual skill in that category.\n\nPlots the top n skills by their occurrence count.\n\nWith the parameters below, we are displaying the top 10 most common skills for the Job_title category.\n\nskill_pd = plot_skills_per_category(cv_skills, \"Job_title\", top_n=10)\n\n\n\n\n\n\n\n\n\nTop N Skills for a Given Job Title\nThis function shows the most common &lt;skills&gt; for candidates who have a specific &lt;job_title&gt;. It simply:\n\nPicks out all candidates with the chosen job title.\n\nCollects their skills of the specified category.\n\nCounts how often each skill appears.\n\nPlots the top n skills by frequency.\n\nWith the parameters below, we are displaying the top 10 most frequent IT skills among candidates with “Commis Chef (m/f)” job experience.\n\nplot_top_skills_for_job_title(cv_skills, \"Commis Chef (m/f)\", \"IT_Skill\", top_n=20)\n\n\n\n\n\n\n\n\n\nplot_top_skills_for_job_title(\n    cv_skills, \"Commis Chef (m/f)\", \"Professional_Skill\", top_n=20\n)\n\n\n\n\n\n\n\n\n\nplot_top_skills_for_job_title(\n    cv_skills, \"Commis Chef (m/f)\", \"Language_Skill\", top_n=10\n)\n\n\n\n\n\n\n\n\n\nplot_top_skills_for_job_title(cv_skills, \"Commis Chef (m/f)\", \"DRIVERSLIC\", top_n=10)\n\n\n\n\n\n\n\n\n\n\n\nCounting Skills per Candidate\nIn this step, we aim to:\n\nVisualize the distribution of the number of skills extracted per candidate.\nSpot and investigate outliers, candidates who list an unusually high number of skills.\n\n\nskill_counts = cv_skills.group_by(\"CANDIDATE_ID\").len()\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(skill_counts[\"len\"], bins=40, edgecolor=\"black\")\nax.set_title(\"Distribution of Number of Skills per Candidate\", fontsize=14, pad=10)\nax.set_xlabel(\"Number of Skills\", fontsize=12)\nax.set_ylabel(\"Number of Candidates\", fontsize=12)\nax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.7, alpha=0.7)\n\n\n\n\n\n\n\n\n\nplot_boxplot(\n    data=skill_counts[\"len\"],\n    labels=None,\n    title=\"Boxplot of Skills per Candidate\",\n    xlabel=\"Number of Skills\",\n    colors=[\"orange\"],\n    figsize=(10, 2),\n)\n\n\n\n\n\n\n\n\n\n\nHard vs Soft Skills Analysis\nIn this section, we investigate the distribution of hard and soft skills extracted from the candidate CVs. (column Professional_Skill)\n\nThe logic used to label each skill as hard or soft is documented in the hard_soft_labelling.ipynb notebook.\n\nHere we present: - Total distribution of hard vs soft skills across all candidates. - Distribution per candidate: number of hard vs soft skills per individual, to highlight representation gaps.\n\nhard_soft_skills = load_data(HARD_SOFT_SKILLS)\ndisplay(hard_soft_skills)\n\n\nshape: (2_827, 2)\n\n\n\nSkill\nlabel\n\n\nstr\nstr\n\n\n\n\n\"V-Ray\"\n\"Hard\"\n\n\n\"Demographics\"\n\"Hard\"\n\n\n\"Angel Investing\"\n\"Hard\"\n\n\n\"Confined Spaces\"\n\"Hard\"\n\n\n\"Target Market Selection\"\n\"Unknown\"\n\n\n…\n…\n\n\n\"Emergency Management\"\n\"Soft\"\n\n\n\"Search Engine Optimization\"\n\"Hard\"\n\n\n\"Personal Branding\"\n\"Soft\"\n\n\n\"Health Products\"\n\"Hard\"\n\n\n\"Legal Consultancy\"\n\"Hard\"\n\n\n\n\n\n\n\ncv_skills_with_label = cv_skills.join(hard_soft_skills, on=\"Skill\")\ncv_skills_with_label\n\n\nshape: (67_873, 4)\n\n\n\nCANDIDATE_ID\nSkill\nSkill_Type\nlabel\n\n\ni64\nstr\nstr\nstr\n\n\n\n\n7974050\n\"Billing Processes\"\n\"Professional_Skill\"\n\"Hard\"\n\n\n7974050\n\"Databases\"\n\"Professional_Skill\"\n\"Hard\"\n\n\n7974050\n\"Polling\"\n\"Professional_Skill\"\n\"Hard\"\n\n\n7965670\n\"Accounting\"\n\"Professional_Skill\"\n\"Hard\"\n\n\n7965670\n\"Administrative Operations\"\n\"Professional_Skill\"\n\"Soft\"\n\n\n…\n…\n…\n…\n\n\n18233\n\"Knowledge of Finance\"\n\"Professional_Skill\"\n\"Hard\"\n\n\n18233\n\"Maintenance\"\n\"Professional_Skill\"\n\"Hard\"\n\n\n18233\n\"Multi-Level Marketing\"\n\"Professional_Skill\"\n\"Unknown\"\n\n\n18233\n\"Sales\"\n\"Professional_Skill\"\n\"Soft\"\n\n\n18233\n\"Telecommunications\"\n\"Professional_Skill\"\n\"Hard\"\n\n\n\n\n\n\n\nHard skills dominate the dataset, accounting for roughly ~75% of all skills extracted from the CVs.\nSoft skills are markedly under represented, at about one skill in five.\n\n\nA small remainder is classified as “Unknown” terms that did not match either taxonomy, highlighting the presence of noise in the parsed skills data. (see hard_soft_labelling.ipynb)\n\n\ncounts = cv_skills_with_label[\"label\"].value_counts()\nplot_distribution_bar(\n    counts,\n    \"label\",\n    \"count\",\n    \"Label\",\n    \"Frequency\",\n    \"Absolute frequency for Hard/Soft skills\",\n)\n\n\n\n\n\n\n\n\n\nper_cand = (\n    cv_skills_with_label.group_by([\"CANDIDATE_ID\", \"label\"])\n    .agg(pl.len())\n    .pivot(\n        index=\"CANDIDATE_ID\",\n        on=\"label\",\n        values=\"len\",\n    )\n    .fill_null(0)\n    .with_columns(\n        [\n            (pl.col(\"Hard\") + pl.col(\"Soft\") + pl.col(\"Unknown\").fill_null(0)).alias(\n                \"total\"\n            ),\n        ]\n    )\n)\n\nAs we can see from the analysis below:\n\nMost applicants (about two thirds) mention at least one hard and one soft skill, suggesting reasonably balanced self presentation.\nOne third list exclusively hard skills; they highlight only technical competence.\nSoft skill only CVs are extremely rare; almost nobody relies on soft skills without pairing them with technical ones.\n\nA negligible fraction provide no skills at all, indicating either very short resumes or parsing errors that deserve inspection.\n\ncats = per_cand.with_columns(\n    [\n        pl.when((pl.col(\"Hard\") == 0) & (pl.col(\"Soft\") == 0))\n        .then(pl.lit(\"No skills\"))\n        .when((pl.col(\"Hard\") &gt; 0) & (pl.col(\"Soft\") == 0))\n        .then(pl.lit(\"Only hard\"))\n        .when((pl.col(\"Hard\") == 0) & (pl.col(\"Soft\") &gt; 0))\n        .then(pl.lit(\"Only soft\"))\n        .otherwise(pl.lit(\"Both\"))\n        .alias(\"category\")\n    ]\n)\n\ncounts = cats.group_by(\"category\").len().sort(\"len\", descending=True)\n\nplot_distribution_bar(\n    counts, \"category\", \"len\", \"Label\", \"Frequency\", \"Frequency for Hard/Soft skills\"\n)\n\n\n\n\n\n\n\n\n\nper_cand = per_cand.with_columns(\n    (pl.col(\"Hard\") / pl.col(\"total\")).alias(\"hard_share\"),\n)\nper_cand\n\n\nshape: (7_149, 6)\n\n\n\nCANDIDATE_ID\nSoft\nUnknown\nHard\ntotal\nhard_share\n\n\ni64\nu32\nu32\nu32\nu32\nf64\n\n\n\n\n136810\n2\n1\n9\n12\n0.75\n\n\n3977219\n7\n2\n15\n24\n0.625\n\n\n6763122\n1\n0\n4\n5\n0.8\n\n\n6829028\n2\n2\n17\n21\n0.809524\n\n\n6606921\n4\n1\n6\n11\n0.545455\n\n\n…\n…\n…\n…\n…\n…\n\n\n5783260\n0\n0\n6\n6\n1.0\n\n\n5768159\n0\n0\n1\n1\n1.0\n\n\n6784314\n2\n0\n0\n2\n0.0\n\n\n6176014\n0\n0\n1\n1\n1.0\n\n\n5547718\n0\n0\n1\n1\n1.0\n\n\n\n\n\n\nThe chart shows how technical competences (hard skills) are distributed across each candidate.\nFor each one of them we calculate the hard skill share, the ratio between the number of hard skills and the total number of skills, listed (hard + soft + any unknown items) and we plot all of these percentages in a histogram.\n\ndata = per_cand[\"hard_share\"].to_numpy()\n\nplt.figure(figsize=(8, 4))\nplt.hist(data, bins=20, edgecolor=\"black\")\nplt.xlabel(\"Share of hard skills per CV\")\nplt.ylabel(\"Number of candidates\")\nplt.title(\"Distribution of Hard-Skill Share Across CVs\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe box plot highlights several outliers. (20+ hard skills / 10+ soft skills)\nSuch counts are well beyond the typical range and may indicate parsing errors (e.g bullet points misclassified as skills, the same skill split into multiple tokens ecc). These outliers should be reviewed manually to confirm whether they reflect genuine, unusually rich profiles or artefacts produced by the parsing pipeline.\n\nplot_boxplot(\n    data=[per_cand[\"Hard\"], per_cand[\"Soft\"]],\n    labels=[\"Hard\", \"Soft\"],\n    title=\"Hard vs Soft Skill Distribution\",\n    xlabel=\"Number of Skills\",\n    colors=[\"#1f77b4\", \"#ff7f0e\"],\n    figsize=(6, 4),\n)"
  },
  {
    "objectID": "notebooks/bias_detection.html",
    "href": "notebooks/bias_detection.html",
    "title": "Bias Detection",
    "section": "",
    "text": "Main Objective:\nDetect errors and biases introduced by the CV parser when extracting skills from raw CV text. We perform this inspection using both rule based (regular expressions) and semantic techniques.\n\nError detection steps:\n\nIdentify errors in candidates Driving Licenses and Language Skills using Regex.\n\nUncover errors in candidates Job Experience using exact matching and semantic approach.\n\nBias detection:\n\nAnalyze the errors identified in Step 1 for the groups previously examined (see distribution_analysis.ipynb) to determine whether the parser has disadvantaged or advantaged any of them.\n\n\n\n%load_ext autoreload \n%autoreload 2\n\nimport json\nimport os\n\nimport polars as pl\nfrom huggingface_hub import login\n\nfrom hiring_cv_bias.bias_detection.fuzzy.matcher import SemanticMatcher\nfrom hiring_cv_bias.bias_detection.fuzzy.parser import JobParser\nfrom hiring_cv_bias.bias_detection.rule_based.data import (\n    add_demographic_info,\n)\nfrom hiring_cv_bias.bias_detection.rule_based.evaluation.compare_parser import (\n    compute_candidate_coverage,\n)\nfrom hiring_cv_bias.bias_detection.rule_based.extractors import (\n    extract_driver_license,\n    extract_languages,\n    norm_driver_license,\n    norm_languages,\n)\nfrom hiring_cv_bias.bias_detection.rule_based.patterns import (\n    driver_license_pattern_eng,\n    jobs_pattern,\n    languages_pattern_eng,\n    normalized_jobs,\n)\nfrom hiring_cv_bias.bias_detection.rule_based.utils import (\n    print_highlighted_cv,\n    print_report,\n)\nfrom hiring_cv_bias.config import (\n    CANDIDATE_CVS_TRANSLATED_CLEANED_PATH,\n    CLEANED_REVERSE_MATCHING_PATH,\n    CLEANED_SKILLS,\n    DRIVING_LICENSE_FALSE_NEGATIVES_PATH,\n    JOB_TITLE_FALSE_NEGATIVES_PATH,\n    LANGUAGE_SKILL_FALSE_NEGATIVES_PATH,\n)\nfrom hiring_cv_bias.utils import load_data\n\npl.Config.set_tbl_cols(-1)\npl.Config.set_tbl_width_chars(200);\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\nwith open(\"token.json\", \"r\") as token:\n    login(token=json.load(token)[\"token\"])\n\n!python -m spacy download en_core_web_sm \n\n\nCollecting en-core-web-sm==3.8.0\n\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 2.2 MB/s eta 0:00:00a 0:00:01m\n\n✔ Download and installation successful\n\nYou can now load the package via spacy.load('en_core_web_sm')\n\n\n\n\n\nLoad the data\n\ndf_cv_raw = load_data(CANDIDATE_CVS_TRANSLATED_CLEANED_PATH)\ndf_skills = load_data(CLEANED_SKILLS)\ndf_info_candidates = load_data(CLEANED_REVERSE_MATCHING_PATH)\n\n\ndf_info_candidates = df_info_candidates.with_columns(\n    pl.when(pl.col(\"LATITUDE\") &gt; 44.5)\n    .then(pl.lit(\"NORTH\"))\n    .when(pl.col(\"LATITUDE\") &lt; 42)\n    .then(pl.lit(\"SOUTH\"))\n    .otherwise(pl.lit(\"CENTER\"))\n    .alias(\"Location\")\n)\n\ndf_cv_raw = df_cv_raw.with_columns(\n    pl.when(pl.col(\"len_anon\") &lt; 1000)\n    .then(pl.lit(\"SHORT\"))\n    .when(pl.col(\"len_anon\") &lt; 2500)\n    .then(pl.lit(\"MEDIUM\"))\n    .otherwise(pl.lit(\"LONG\"))\n    .alias(\"length\")\n)\n\n\n\nBias detection for Driver Licences\n\nPre-processing step –&gt; driving licence flag\nWe call add_demographic_info() to add a Boolean column, has_driving_license, to the CV dataframe. This flag will help us compare what the regex detects in the raw CV text with what the parser extracted as driving licence for each candidate, allowing us to identify potential omissions in the parsing step.\nHow the flag is generated\n\nA single case insensitive regex (driver_license_pattern_eng) looks for common phrases such as “driving license B”, “C1 driving licence” or even “own car”.\n\nThe helper function extract_driver_license(text) returns True if the regex matches anywhere in the CV text.\n\nResulting columns in df_cv\n\nSame as before\nGender, Location —&gt; from the candidates sheet\nhas_driving_license —&gt; True if any licence is mentioned, otherwise False\n\nNote\nFor now we only care whether a candidate has any licence. (given that the driver license type column contains a handful of null values (see data_cleaning.ipynb))\nThe same regex already captures specific categories (A, B, C…), so the analysis could be extended later if we want to explore potential biases tied to particular licence types.\n\n\ndf_cv = add_demographic_info(df_cv_raw, df_info_candidates)\ndf_cv.head()\n\n\nshape: (5, 8)\n\n\n\nCANDIDATE_ID\nCV_text_anon\nTranslated_CV\nlen_anon\nlength\nGender\nLocation\nhas_driving_license\n\n\ni64\nstr\nstr\ni64\nstr\nstr\nstr\nbool\n\n\n\n\n7990324\n\"CV anonimizzato: \"\"\" PROFILO D…\n\" profile graduated from elsa m…\n1445\n\"MEDIUM\"\n\"Female\"\n\"NORTH\"\ntrue\n\n\n7974050\n\"CV anonimizzato: \"\"\" Curricul…\n\" curriculum vitae personal inf…\n2148\n\"MEDIUM\"\n\"Female\"\n\"CENTER\"\ntrue\n\n\n7965670\n\"CV anonimizzato: \"\"\" ESPERIENZ…\n\" work experience 03/27/2023 – …\n4911\n\"LONG\"\n\"Female\"\n\"NORTH\"\ntrue\n\n\n7960501\n\"CV anonimizzato: \"\"\" Esperienz…\n\" work experience waiter and ba…\n680\n\"SHORT\"\n\"Male\"\n\"NORTH\"\nfalse\n\n\n7960052\n\"CV anonimizzato: \"\"\"       Dat…\n\" date of birth: 03/26/1996 nat…\n5913\n\"LONG\"\n\"Female\"\n\"CENTER\"\nfalse\n\n\n\n\n\n\nComparing parser output and regex detection\nThe compute_candidate_coverage() function evaluates how well the parsing system detects a specific category of skills by comparing it to our approach.\nIn this case, the chosen category is \"DRIVERSLIC\" and we use a custom regex based extractor applied directly to the raw CV text.\nThis step is crucial for measuring the parser’s coverage by quantifying false negatives. (skills that are mentioned in the CV but missed by the parser)\nOutput breakdown: 1. Regex positive candidates: number of unique candidates flagged by our rule based extractor. 2. Parser positive unique candidates: number of unique candidates flagged by the parser.\n\nBoth regex & parser: candidates detected by both methods.\nOnly regex: candidates our regex caught but the parser missed.\nOnly parser: candidates the parser flagged but our regex did not.\n\nThen print_report() displays: - The overall confusion matrix and derived metrics (accuracy, precision, recall, F1).\n\nres_dl = compute_candidate_coverage(\n    df_cv=df_cv,\n    df_parser=df_skills,\n    skill_type=\"DRIVERSLIC\",\n    extractor=extract_driver_license,\n    norm=norm_driver_license,\n)\n\nprint(\"Confusion matrix:\", res_dl.conf)\n\n\n\n\nRegex positive candidates        : 4134\nParser positive unique candidates: 2032\n- Both regex & parser   : 1689\n- Only regex            : 2445\n- Only parser           : 343\n\nConfusion matrix: \nCounts   -&gt; TP:1689   FP:343   TN:2927   FN:2445 \nMetrics  -&gt; Precision:0.83  Recall:0.409  F1:0.548  Acc:0.623\n\n\nThe parser achieves high precision (~83 %) but low recall (~41 %).\nIn other words, when it flags a skill it is usually correct, yet it misses more than half of the skills that the regex finds.\nLet’s dive into false negatives (FN)\n* We’ll highlight in red the exact terms captured by the regex directly inside the CV text, making it easy to verify their presence at a glance.\n\ndf_fn = pl.DataFrame(res_dl.fn_rows)\nsample = df_fn.sample(n=2, shuffle=True)\nfor row in sample.to_dicts():\n    print_highlighted_cv(row, pattern=driver_license_pattern_eng)\n\n\nCANDIDATE ID: 4454222 - GENERE: Female\n\nReason: Rule-based extractor found skill but parser missed it.\n\n--------------------------------------------------------------------------------\n\naching of political economy in english. driving license: b professional skills: customer orientati\n\n--------------------------------------------------------------------------------\n\n\n\nCANDIDATE ID: 4087594 - GENERE: Male\n\nReason: Rule-based extractor found skill but parser missed it.\n\n--------------------------------------------------------------------------------\n\nd in the field of technical maintenance driving license a1, b further information despite holdi\n\nfurther information despite holding the license b, at the moment I do not have a car\n\n--------------------------------------------------------------------------------\n\n\n\n\n\nprint(f\"False negatives matching snippet pattern: {df_fn.height}\")\ndf_fn.write_csv(DRIVING_LICENSE_FALSE_NEGATIVES_PATH, separator=\";\")\nprint(\"Saved filtered false negatives!\")\n\nFalse negatives matching snippet pattern: 2445\nSaved filtered false negatives!\n\n\n\n%%bash --bg \ncd ..\n\n# for Unix users  \n.venv/bin/python -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n\n# for Windows users\n#.venv/Scripts/python.exe -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n\n\n\nBias Detection Metrics\nTo assess the model’s fairness, we employed the following bias detection metrics:\n\n\n\n\n\n\n\n\nMetric\nFormula\nInterpretation\n\n\n\n\nEquality of Opportunity (TPR parity)\n\\[\\text{TPR}_g = \\frac{TP_g}{TP_g + FN_g} \\]\n\\(\\text{TPR}_g\\) equal for every \\(g\\) ensures that every individual who truly qualifies for a positive outcome has the same chance of being correctly identified, regardless of group membership.\n\n\n Calibration (NPV)\n\\[\\text{NPV}_g = \\frac{TN_g}{TN_g + FN_g}\\qquad \\]\n\\(\\text{NPV}_g\\) parity for every \\(g\\) ensures that when the model predicts a negative outcome, the probability of being correct is the same for every group.\n\n\n Selection Rate\n\\[\\text{SR}_g = \\frac{TP_g + FP_g}{TP_g + FP_g + TN_g + FN_g} \\]\nShare of individuals in group \\(g\\) predicted positive (selected).\n\n\n Disparate Impact (DI)\n\\[\\displaystyle DI = \\frac{\\text{SR}_{\\text{target}}}{\\text{SR}_{\\text{reference}}}\\]\nRatio of selection rates; values &lt; 0.80 (four-fifths rule) indicate potential adverse impact against the target group.\n\n\n\nAll these metrics were computed for the Gender and Location groups to detect and quantify possible bias in the selection process.\n\nprint_report(\n    result=res_dl,\n    df_population=df_cv,\n    reference_col=\"Male\",\n    group_col=\"Gender\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 1689, FP: 343, TN: 2927, FN: 2445\nAccuracy: 0.623, Precision: 0.831, Recall: 0.409, F1: 0.548\n\nError and rates by Gender:\n\n\n\n\nshape: (2, 12)\n\n\n\nGender\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Male\"\n3984\n966\n189\n1316\n1513\n3984\n0.04744\n0.330321\n0.423313\n0.534818\n1.0\n\n\n\"Female\"\n3420\n723\n154\n1129\n1414\n3420\n0.045029\n0.330117\n0.390389\n0.556036\n0.884526\n\n\n\n\n\n\n\nprint_report(\n    result=res_dl,\n    df_population=df_cv,\n    reference_col=\"NORTH\",\n    group_col=\"Location\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 1689, FP: 343, TN: 2927, FN: 2445\nAccuracy: 0.623, Precision: 0.831, Recall: 0.409, F1: 0.548\n\nError and rates by Location:\n\n\n\n\nshape: (3, 12)\n\n\n\nLocation\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SOUTH\"\n1060\n229\n51\n341\n439\n1060\n0.048113\n0.321698\n0.401754\n0.562821\n0.968674\n\n\n\"NORTH\"\n5343\n1217\n240\n1789\n2097\n5343\n0.044919\n0.334831\n0.404857\n0.539629\n1.0\n\n\n\"CENTER\"\n1001\n243\n52\n315\n391\n1001\n0.051948\n0.314685\n0.435484\n0.553824\n1.080721\n\n\n\n\n\n\n\nprint_report(\n    result=res_dl,\n    df_population=df_cv,\n    reference_col=\"LONG\",\n    group_col=\"length\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 1689, FP: 343, TN: 2927, FN: 2445\nAccuracy: 0.623, Precision: 0.831, Recall: 0.409, F1: 0.548\n\nError and rates by length:\n\n\n\n\nshape: (3, 12)\n\n\n\nlength\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"MEDIUM\"\n3005\n698\n143\n872\n1292\n3005\n0.047587\n0.290183\n0.444586\n0.597043\n0.998508\n\n\n\"LONG\"\n3789\n901\n161\n1470\n1257\n3789\n0.042491\n0.387965\n0.380008\n0.460946\n1.0\n\n\n\"SHORT\"\n610\n90\n39\n103\n378\n610\n0.063934\n0.168852\n0.466321\n0.785863\n0.754501\n\n\n\n\n\n\n\n\nBias detection for Language Skill\nJust as we applied a simple presence check for driving licenses, we handle languages with a more granular, ad-hoc normalization and extraction pipeline that recognizes each specific language individually rather than merely flagging “has any language”.\nMain steps for doing this are:\n\nBuild a reverse lookup map (_reverse_language_map) by iterating over each language code in LANGUAGE_VARIANTS (populated from pycountry with English name variants alpha_2) and all its known name variants, storing entries like “english” -&gt; “en”, “italian” -&gt; “it”, etc.\nApply norm_languagesto every extracted mention from our regex based extractor so that each occurrence (like “English B2” is mapped to a clean ISO code (“en”).\nOnce all language mentions have been normalized to ISO codes via norm_languages, we invoke the coverage routine to quantify how well the parser matches our “ground truth” extractions.\n\n\nres_lg = compute_candidate_coverage(\n    df_cv=df_cv,\n    df_parser=df_skills,\n    skill_type=\"Language_Skill\",\n    extractor=extract_languages,\n    norm=norm_languages,\n)\n\nprint(\"Confusion matrix:\", res_lg.conf)\n\n\n\n\nRegex positive candidates        : 6387\nParser positive unique candidates: 6500\n- Both regex & parser   : 5615\n- Only regex            : 772\n- Only parser           : 885\n\nConfusion matrix: \nCounts   -&gt; TP:10389  FP:2392  TN:132    FN:5438 \nMetrics  -&gt; Precision:0.81  Recall:0.656  F1:0.726  Acc:0.573\n\n\n\ndf_fn = pl.DataFrame(res_lg.fn_rows)\nsample = df_fn.sample(n=2, shuffle=True)\nfor row in sample.to_dicts():\n    print_highlighted_cv(row, pattern=languages_pattern_eng)\n\n\nCANDIDATE ID: 5512414 - GENERE: Male\n\nReason: Rule-based extractor found skill but parser missed it.\n\n--------------------------------------------------------------------------------\n\n nationality: Italian date of birth: 30 Apr 1996 work experie\n\nItaly) language skills native language: Italian other languages: English French listeni\n\ntive language: Italian other languages: English French listening c2 reading c2 writing \n\nguage: Italian other languages: English French listening c2 reading c2 writing c1 list\n\n--------------------------------------------------------------------------------\n\n\n\nCANDIDATE ID: 4675569 - GENERE: Male\n\nReason: Rule-based extractor found skill but parser missed it.\n\n--------------------------------------------------------------------------------\n\neation of metadata and links. certified languages italian german english civil service (completed\n\n and links. certified languages italian german english civil service (completed) 2019 \n\nnks. certified languages italian german english civil service (completed) 2019 spanish \n\n english civil service (completed) 2019 spanish participation in the development of the\n\n--------------------------------------------------------------------------------\n\n\n\n\n\nprint(f\"False negatives matching snippet pattern: {df_fn.height}\")\ndf_fn.write_csv(LANGUAGE_SKILL_FALSE_NEGATIVES_PATH, separator=\";\")\nprint(\"Saved filtered false negatives to false_negative.csv\")\n\nFalse negatives matching snippet pattern: 5438\nSaved filtered false negatives to false_negative.csv\n\n\n\n%%bash --bg \ncd ..\n# for Unix users\n.venv/bin/python -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n\n# for Windows users\n#.venv/Scripts/python.exe -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n\n\nprint_report(\n    result=res_lg,\n    df_population=df_cv,\n    reference_col=\"Male\",\n    group_col=\"Gender\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 10389, FP: 2392, TN: 132, FN: 5438\nAccuracy: 0.573, Precision: 0.813, Recall: 0.656, F1: 0.726\n\nError and rates by Gender:\n\n\n\n\nshape: (2, 12)\n\n\n\nGender\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Male\"\n3984\n5420\n1251\n2608\n77\n9356\n0.133711\n0.278752\n0.675137\n0.028678\n1.0\n\n\n\"Female\"\n3420\n4969\n1141\n2830\n55\n8995\n0.126848\n0.314619\n0.637133\n0.019064\n0.952663\n\n\n\n\n\n\n\nprint_report(\n    result=res_lg,\n    df_population=df_cv,\n    reference_col=\"NORTH\",\n    group_col=\"Location\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 10389, FP: 2392, TN: 132, FN: 5438\nAccuracy: 0.573, Precision: 0.813, Recall: 0.656, F1: 0.726\n\nError and rates by Location:\n\n\n\n\nshape: (3, 12)\n\n\n\nLocation\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SOUTH\"\n1060\n1474\n348\n791\n14\n2627\n0.13247\n0.301104\n0.650773\n0.017391\n0.997271\n\n\n\"NORTH\"\n5343\n7484\n1701\n3914\n108\n13207\n0.128795\n0.296358\n0.656606\n0.026852\n1.0\n\n\n\"CENTER\"\n1001\n1431\n343\n733\n10\n2517\n0.136273\n0.29122\n0.661275\n0.013459\n1.013434\n\n\n\n\n\n\n\nprint_report(\n    result=res_lg,\n    df_population=df_cv,\n    reference_col=\"LONG\",\n    group_col=\"length\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 10389, FP: 2392, TN: 132, FN: 5438\nAccuracy: 0.573, Precision: 0.813, Recall: 0.656, F1: 0.726\n\nError and rates by length:\n\n\n\n\nshape: (3, 12)\n\n\n\nlength\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SHORT\"\n610\n489\n421\n219\n27\n1156\n0.364187\n0.189446\n0.690678\n0.109756\n1.19194\n\n\n\"LONG\"\n3789\n5715\n925\n3371\n43\n10054\n0.092003\n0.335289\n0.62899\n0.012595\n1.0\n\n\n\"MEDIUM\"\n3005\n4185\n1046\n1848\n62\n7141\n0.146478\n0.258787\n0.693685\n0.032461\n1.109166\n\n\n\n\n\n\n\n\nBias detection for Job Title\nLoading list of jobs from ESCO and filtering out those that are too specific (length &gt; 3).\n\ndf_skills_cleaned = df_skills.with_columns(\n    pl.col(\"Skill\")\n    .str.to_lowercase()\n    .str.replace_all(\"(m/f)\", \"\", literal=True)\n    .str.strip_chars()\n    .alias(\"Skill\")\n)\n\nThe bias detection pipeline for job titles consists of two main components:\n\nJobParser: a class that extracts job experiences listed in ESCO from raw CV texts, using SpaCy’s PhraseMatcher.\nSemanticMatcher: once job experiences are extracted, the SemanticMatcher (all-MiniLM-L6-v2) is then used exclusively to determine which of these experiences match the parser extracted skills, employing semantic embeddings. Pairwise cosine similarity is calculated between the embeddings of JobParser skills and Parser skills. Matches are established when this similarity exceeds a specified threshold. This matching step is used only to compute metrics.\n\n\nparser = JobParser(normalized_jobs)\nmatcher = SemanticMatcher()\n\n\nres_job = compute_candidate_coverage(\n    df_cv,\n    df_skills_cleaned,\n    \"Job_title\",\n    parser.parse_with_n_grams,\n    matcher=matcher.semantic_comparison,\n)\n\nprint(\"Confusion matrix:\", res_job.conf)\n\n\n\n\nRegex positive candidates        : 5614\nParser positive unique candidates: 6763\n- Both regex & parser   : 5299\n- Only regex            : 315\n- Only parser           : 1464\n\nConfusion matrix: \nCounts   -&gt; TP:7838   FP:14976 TN:326    FN:6394 \nMetrics  -&gt; Precision:0.34  Recall:0.551  F1:0.423  Acc:0.276\n\n\n\ndf_fn = pl.DataFrame(res_job.fn_rows)\nsample = df_fn.sample(n=2, shuffle=True)\nfor row in sample.to_dicts():\n    print_highlighted_cv(row, pattern=jobs_pattern)\n\n\nCANDIDATE ID: 6538036 - GENERE: Female\n\nReason: Rule-based extractor found skill but parser missed it.\n\n--------------------------------------------------------------------------------\n\ne invoices in Italy, intra-European and extra-European countries. VAT settlements, re\n\nttlements, relationship management with accountant and auditor. administrative / accountin\n\nd tax obligations to be provided to the accountant for electronic submissions. electronic \n\n--------------------------------------------------------------------------------\n\n\n\nCANDIDATE ID: 6591312 - GENERE: Female\n\nReason: Rule-based extractor found skill but parser missed it.\n\n--------------------------------------------------------------------------------\n\nmail] professional experience 2017–2018 secretary le sartoriali del viaggio, cremona (ita\n\n15–2016 graphics (intern) daniele oneta-photographer, cremona (italy) use of graphic softwar\n\n photo retouching 20/06/2018–01/08/2018 secretary car dealership chiusi, cremona (italy) \n\ny, document checking 10/08/2018–to date waiter/waitress old wild west (cigierre) via c\n\nment checking 10/08/2018–to date waiter/waitress old wild west (cigierre) via castelleon\n\n 26100 cremona (italy) I have worked as waitress, drinks counter, cash register and cust\n\n--------------------------------------------------------------------------------\n\n\n\n\n\nprint(f\"False negatives matching snippet pattern: {df_fn.height}\")\ndf_fn.write_csv(JOB_TITLE_FALSE_NEGATIVES_PATH, separator=\";\")\nprint(\"Saved filtered false negatives to false_negative.csv\")\n\nFalse negatives matching snippet pattern: 6394\nSaved filtered false negatives to false_negative.csv\n\n\n\n%%bash --bg \ncd ..\n\n# for Unix users\n.venv/bin/python -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n\n# for Windows users\n#.venv/Scripts/python.exe -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n\n\nprint_report(\n    result=res_job,\n    df_population=df_cv,\n    reference_col=\"Male\",\n    group_col=\"Gender\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 7838, FP: 14976, TN: 326, FN: 6394\nAccuracy: 0.276, Precision: 0.344, Recall: 0.551, F1: 0.423\n\nError and rates by Gender:\n\n\n\n\nshape: (2, 12)\n\n\n\nGender\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Female\"\n3420\n4265\n6534\n3273\n111\n14183\n0.460692\n0.230769\n0.5658\n0.032801\n0.972811\n\n\n\"Male\"\n3984\n3573\n8442\n3121\n215\n15351\n0.549932\n0.203309\n0.533762\n0.064448\n1.0\n\n\n\n\n\n\n\nprint_report(\n    result=res_job,\n    df_population=df_cv,\n    reference_col=\"NORTH\",\n    group_col=\"Location\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 7838, FP: 14976, TN: 326, FN: 6394\nAccuracy: 0.276, Precision: 0.344, Recall: 0.551, F1: 0.423\n\nError and rates by Location:\n\n\n\n\nshape: (3, 12)\n\n\n\nLocation\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SOUTH\"\n1060\n1078\n1956\n937\n62\n4033\n0.484999\n0.232333\n0.534988\n0.062062\n0.969529\n\n\n\"CENTER\"\n1001\n1211\n2130\n938\n36\n4315\n0.493627\n0.217381\n0.563518\n0.036961\n0.997859\n\n\n\"NORTH\"\n5343\n5549\n10890\n4519\n228\n21186\n0.514019\n0.213301\n0.551152\n0.04803\n1.0\n\n\n\n\n\n\n\nprint_report(\n    result=res_job,\n    df_population=df_cv,\n    reference_col=\"LONG\",\n    group_col=\"length\",\n    metrics=[\n        \"equality_of_opportunity\",\n        \"calibration_npv\",\n    ],\n)\n\nTP: 7838, FP: 14976, TN: 326, FN: 6394\nAccuracy: 0.276, Precision: 0.344, Recall: 0.551, F1: 0.423\n\nError and rates by length:\n\n\n\n\nshape: (3, 12)\n\n\n\nlength\ntotal\ntp\nfp\nfn\ntn\ntotal_skills\nfp_rate\nfn_rate\nequality_of_opportunity\ncalibration_npv\ndisparate_impact\n\n\nstr\nu32\nu32\nu32\nu32\nu32\nu32\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"LONG\"\n3789\n5212\n9071\n4065\n62\n18410\n0.492721\n0.220804\n0.56182\n0.015023\n1.0\n\n\n\"SHORT\"\n610\n266\n783\n254\n97\n1400\n0.559286\n0.181429\n0.511538\n0.276353\n0.965788\n\n\n\"MEDIUM\"\n3005\n2360\n5122\n2075\n167\n9724\n0.526738\n0.21339\n0.532131\n0.074487\n0.991761"
  }
]