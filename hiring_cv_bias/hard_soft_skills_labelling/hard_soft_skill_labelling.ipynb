{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Soft skills labelling\n",
    "\n",
    "**Main Objective:**  \n",
    "\n",
    "This notebook automates the labeling of extracted `Professional_Skill` entries as **Hard**, **Soft**, or **Unknown**. \n",
    "\n",
    "We will do this using a **4-bit quantized Llama-3.1 8B Instruct model** on a GPU P100. \n",
    "\n",
    "**Steps**:\n",
    "1. It loads the list of unique skills\n",
    "2. **Classifies** each skill in batches via the language model\n",
    "3. **Evaluate** the model's accuracy using the technique **LLM as a judge** via Chat Gpt o3 model.\n",
    "4. Writes the results to a CSV.\n",
    "\n",
    "Additionally, we include the overall breakdown, the model’s Chain of Thought and each skill’s individual classification in the `CoT.log` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from hiring_cv_bias.config import CLEANED_SKILLS, HARD_SOFT_SKILLS\n",
    "from hiring_cv_bias.hard_soft_skills_labelling.utils import (\n",
    "    batch_classify_skills,\n",
    "    clean_results,\n",
    ")\n",
    "from hiring_cv_bias.utils import load_data\n",
    "\n",
    "SEED = 42\n",
    "login(token=\"[YOUR_TOKEN]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_skills = load_data(CLEANED_SKILLS)\n",
    "skills = (\n",
    "    cv_skills.filter(pl.col(\"Skill_Type\") == \"Professional_Skill\")[\"Skill\"]\n",
    "    .unique()\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we loads the model (using nf4 and float16) and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `batch_classify_skills` in `utils.py` does the work. Let's breakdown it:\n",
    "\n",
    "**Inputs:**  \n",
    "- `model`: A HuggingFace causal LM model instance (e.g. quantized Llama-3.1-8B Instruct).  \n",
    "- `tokenizer`: Corresponding tokenizer for the model.  \n",
    "- `skills`: List of skill strings to classify.  \n",
    "- `batch_size`: Number of skills to send to the model at once.\n",
    "\n",
    "**Process:**  \n",
    "1. Iterate over the `skills` list in chunks of size `batch_size`.  \n",
    "2. For each batch, construct a prompting template that:  \n",
    "   - Instructs the model to think step by step about the skill.  \n",
    "   - Provides four concrete examples (Data Analysis -> Hard; Communication -> Soft).  \n",
    "\n",
    "3. Tokenize all prompts simultaneously with padding/truncation and move tensors to the model’s device.  \n",
    "4. Call `model.generate(...)` to produce completions (up to 150 new tokens) for each prompt.  \n",
    "5. Decode each generated output, extract the final token as the predicted label (`Hard`, `Soft`, or `Unknown`) and append to `labels`.  \n",
    "6. Return the full list of labels in the same order as the input skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_soft_labels = batch_classify_skills(skills, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hard_soft_labels.count(\"Hard\"),\n",
    "    hard_soft_labels.count(\"Soft\"),\n",
    "    hard_soft_labels.count(\"Unknown\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pl.DataFrame({\"Skill\": skills, \"label\": hard_soft_labels})\n",
    "output_df.write_csv(\"hard_soft_skills.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimated the model's accuracy using **a sample of 100 skills**: 50 predicted as hard skills and 50 predicted as soft skills.\n",
    "\n",
    "We then compared the model's predictions with evaluations provided by ChatGPT o3 model.\n",
    "\n",
    "From this comparison, we derived separate accuracy estimates for hard and soft skills, as well as an overall accuracy score.\n",
    "\n",
    "Accuracy:\n",
    "- **Hard**     --> 49/50\n",
    "- **Soft**     --> 23/50\n",
    "- **Overall**  --> 72/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_soft_df = load_data(HARD_SOFT_SKILLS)\n",
    "print(\n",
    "    hard_soft_df[\"label\"]\n",
    "    .value_counts()\n",
    "    .filter(pl.col(\"label\").is_in([\"Hard\", \"Soft\", \"Unknown\"]))\n",
    "    .sort(pl.col(\"count\"), descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now any label not equal to `Hard`, `Soft` or `Unknown` is replaced with `Unknown`. This step helps correct misclassifications arising from the model’s reasoning (e.g. truncated responses or unexpected formats) since we use the last token of its output as the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_soft_df = clean_results(hard_soft_df)\n",
    "print(\n",
    "    hard_soft_df[\"label\"]\n",
    "    .value_counts()\n",
    "    .filter(pl.col(\"label\").is_in([\"Hard\", \"Soft\", \"Unknown\"]))\n",
    "    .sort(pl.col(\"count\"), descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "- **Hard**     --> 46/50\n",
    "- **Soft**     --> 16/50\n",
    "- **Overall**  --> /100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_skills = (\n",
    "    hard_soft_df.filter(pl.col(\"label\") == \"Hard\")\n",
    "    .sample(50, shuffle=True, seed=SEED)\n",
    "    .to_numpy()\n",
    ")\n",
    "soft_skills = (\n",
    "    hard_soft_df.filter(pl.col(\"label\") == \"Soft\")\n",
    "    .sample(50, shuffle=True, seed=SEED)\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "skills_sample = np.concatenate((hard_skills, soft_skills))\n",
    "np.random.shuffle(skills_sample)\n",
    "skills_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
