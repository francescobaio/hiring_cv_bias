{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Detection\n",
    "\n",
    "**Main Objective:**  \n",
    "Detect errors and biases introduced by the CV parser when extracting skills from raw CV text.\n",
    "We perform this inspection using both rule based (regular expressions) and semantic techniques.\n",
    "\n",
    "1. **Error detection steps:**  \n",
    "    - Identify errors in candidates **Driving Licenses** and **Language Skills** using Regex.  \n",
    "    - Uncover errors in candidates **Job Experience** using exact matching and semantic approach.  \n",
    "\n",
    "2. **Bias detection:**\n",
    "    - Analyze the errors identified in Step 1 for the groups previously examined (see `distribution_analysis.ipynb`) to determine whether the parser has disadvantaged or advantaged any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from huggingface_hub import login\n",
    "from Levenshtein import ratio\n",
    "\n",
    "from hiring_cv_bias.bias_detection.fuzzy.matcher import SemanticMatcher\n",
    "from hiring_cv_bias.bias_detection.fuzzy.parser import JobParser\n",
    "from hiring_cv_bias.bias_detection.rule_based.data import (\n",
    "    add_demographic_info,\n",
    ")\n",
    "from hiring_cv_bias.bias_detection.rule_based.evaluation.compare_parser import (\n",
    "    compute_candidate_coverage,\n",
    ")\n",
    "from hiring_cv_bias.bias_detection.rule_based.extractors import (\n",
    "    extract_driver_license,\n",
    "    extract_languages,\n",
    "    norm_driver_license,\n",
    "    norm_languages,\n",
    ")\n",
    "from hiring_cv_bias.bias_detection.rule_based.patterns import (\n",
    "    driver_license_pattern_eng,\n",
    "    jobs_pattern,\n",
    "    languages_pattern_eng,\n",
    "    normalized_jobs,\n",
    ")\n",
    "from hiring_cv_bias.bias_detection.rule_based.utils import (\n",
    "    print_highlighted_cv,\n",
    "    print_report,\n",
    ")\n",
    "from hiring_cv_bias.bias_detection.utils import extract_skill_cases\n",
    "from hiring_cv_bias.config import (\n",
    "    CANDIDATE_CVS_TRANSLATED_CLEANED_PATH,\n",
    "    CLEANED_REVERSE_MATCHING_PATH,\n",
    "    CLEANED_SKILLS,\n",
    "    DRIVING_LICENSE_FALSE_NEGATIVES_PATH,\n",
    "    GENDER_JOBS_PATH,\n",
    "    JOB_TITLE_FALSE_NEGATIVES_PATH,\n",
    "    LANGUAGE_SKILL_FALSE_NEGATIVES_PATH,\n",
    ")\n",
    "from hiring_cv_bias.utils import load_data\n",
    "\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_tbl_width_chars(200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\n",
    "with open(\"token.json\", \"r\") as token:\n",
    "    login(token=json.load(token)[\"token\"])\n",
    "\n",
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_raw = load_data(CANDIDATE_CVS_TRANSLATED_CLEANED_PATH)\n",
    "df_skills = load_data(CLEANED_SKILLS)\n",
    "df_info_candidates = load_data(CLEANED_REVERSE_MATCHING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_candidates = df_info_candidates.with_columns(\n",
    "    pl.when(pl.col(\"LATITUDE\") > 44.5)\n",
    "    .then(pl.lit(\"NORTH\"))\n",
    "    .when(pl.col(\"LATITUDE\") < 42)\n",
    "    .then(pl.lit(\"SOUTH\"))\n",
    "    .otherwise(pl.lit(\"CENTER\"))\n",
    "    .alias(\"Location\")\n",
    ")\n",
    "\n",
    "df_cv_raw = df_cv_raw.with_columns(\n",
    "    pl.when(pl.col(\"len_anon\") < 1000)\n",
    "    .then(pl.lit(\"SHORT\"))\n",
    "    .when(pl.col(\"len_anon\") < 2500)\n",
    "    .then(pl.lit(\"MEDIUM\"))\n",
    "    .otherwise(pl.lit(\"LONG\"))\n",
    "    .alias(\"length\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias detection for Driver Licences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pre-processing step –> driving licence flag**  \n",
    "  We call `add_demographic_info()` to add a Boolean column, `has_driving_license`, to the CV dataframe. This flag will help us compare what the regex detects in the raw CV text with what the parser extracted as driving licence for each candidate, allowing us to identify potential omissions in the parsing step.\n",
    "\n",
    "- **How the flag is generated**  \n",
    "  - A single case insensitive regex (`driver_license_pattern_eng`) looks for common phrases such as “driving license B”, \"C1 driving licence” or even “own car”.  \n",
    "  - The helper function `extract_driver_license(text)` returns `True` if the regex matches anywhere in the CV text.  \n",
    "\n",
    "- **Resulting columns in `df_cv`**  \n",
    "  -  Same as before\n",
    "  - `Gender`, `Location` —> from the candidates sheet\n",
    "  - `has_driving_license` —> `True` if any licence is mentioned, otherwise `False`\n",
    "\n",
    "- **Note**  \n",
    "  For now we only care whether a candidate has *any* licence. (given that the driver license type column contains a handful of null values (see `data_cleaning.ipynb`))\n",
    "  \n",
    "  The same regex already captures specific categories (A, B, C…), so the analysis could be extended later if we want to explore potential biases tied to particular licence types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = add_demographic_info(df_cv_raw, df_info_candidates)\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing parser output and regex detection**\n",
    "\n",
    "The `compute_candidate_coverage()` function evaluates how well the parsing system detects a specific category of skills  by comparing it to our approach. \n",
    "\n",
    "In this case, the chosen category is  `\"DRIVERSLIC\"` and we use a custom regex based extractor applied directly to the raw CV text.\n",
    "\n",
    "\n",
    "This step is crucial for measuring the parser’s coverage by quantifying **false negatives**. (skills that are mentioned in the CV but missed by the parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output breakdown:** \n",
    "   1. **Regex positive candidates**: number of unique candidates flagged by our rule based extractor.\n",
    "   2. **Parser positive unique candidates**: number of unique candidates flagged by the parser.\n",
    "\n",
    "- **Both regex & parser**: candidates detected by both methods.\n",
    "- **Only regex**: candidates our regex caught but the parser missed. \n",
    "- **Only parser**: candidates the parser flagged but our regex did not.\n",
    "\n",
    "Then `print_report()` displays:\n",
    "   - The overall confusion matrix and derived metrics (accuracy, precision, recall, F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dl = compute_candidate_coverage(\n",
    "    df_cv=df_cv,\n",
    "    df_parser=df_skills,\n",
    "    skill_type=\"DRIVERSLIC\",\n",
    "    extractor=extract_driver_license,\n",
    "    norm=norm_driver_license,\n",
    ")\n",
    "\n",
    "print(\"Confusion matrix:\", res_dl.conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser achieves **high precision (~83 %)** but **low recall (~41 %)**.  \n",
    "In other words, when it flags a skill it is usually correct, yet it **misses more than half of the skills** that the regex finds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive into **false negatives (FN)**  \n",
    "   * We’ll highlight in **red** the exact terms captured by the regex directly inside the CV text, making it easy to verify their presence at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn = pl.DataFrame(res_dl.fn_rows)\n",
    "sample = df_fn.sample(n=2, shuffle=True)\n",
    "for row in sample.to_dicts():\n",
    "    print_highlighted_cv(row, pattern=driver_license_pattern_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"False negatives matching snippet pattern: {df_fn.height}\")\n",
    "df_fn.write_csv(DRIVING_LICENSE_FALSE_NEGATIVES_PATH, separator=\";\")\n",
    "print(\"Saved filtered false negatives!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg \n",
    "cd ..\n",
    "\n",
    "# for Unix users  \n",
    ".venv/bin/python -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n",
    "\n",
    "# for Windows users\n",
    "#.venv/Scripts/python.exe -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Detection Metrics\n",
    "\n",
    "To assess the model’s fairness, we employed the following **bias detection metrics**:\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| **Equality of Opportunity&nbsp;(TPR parity)** | $$\\text{TPR}_g = \\frac{TP_g}{TP_g + FN_g} $$ | $\\text{TPR}_g$ equal for every $g$ ensures that **every individual who truly qualifies** for a positive outcome has the **same chance** of being correctly identified, regardless of group membership. |\n",
    "| <br> **Calibration&nbsp;\\(NPV\\)** | $$\\text{NPV}_g = \\frac{TN_g}{TN_g + FN_g}\\qquad $$ | $\\text{NPV}_g$ parity for every $g$ ensures that **when the model predicts a negative outcome**, the probability of being correct is the **same** for every group. |\n",
    "| <br> **Selection Rate** | $$\\text{SR}_g = \\frac{TP_g + FP_g}{TP_g + FP_g + TN_g + FN_g} $$ | Share of individuals in group $g$ predicted positive (selected). |\n",
    "| <br> **Disparate Impact (DI)** | $$\\displaystyle DI = \\frac{\\text{SR}_{\\text{target}}}{\\text{SR}_{\\text{reference}}}$$ | Ratio of selection rates; values **\\< 0.80** (four-fifths rule) indicate potential adverse impact against the target group. |\n",
    "\n",
    "\n",
    "\n",
    "<u>All these metrics were computed for the **Gender** and **Location** groups to detect and quantify possible bias in the selection process.</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_dl,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"Male\",\n",
    "    group_col=\"Gender\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_dl,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"NORTH\",\n",
    "    group_col=\"Location\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_dl,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"LONG\",\n",
    "    group_col=\"length\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias detection for Language Skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we applied a simple presence check for driving licenses, we handle languages with a more granular, ad-hoc normalization and extraction pipeline that recognizes each specific language individually rather than merely flagging “has any language”.\n",
    "\n",
    "Main **steps** for doing this are:\n",
    "\n",
    "* Build a reverse lookup map (`_reverse_language_map`) by iterating over each language code in `LANGUAGE_VARIANTS` (populated from pycountry with English name variants `alpha_2`) and all its known name variants, storing entries like \"english\" -> \"en\", \"italian\" -> \"it\", etc.\n",
    "* Apply `norm_languages`to every extracted mention from our regex based extractor so that each occurrence (like \"English B2” is mapped to a clean ISO code (\"en\").\n",
    "* Once all language mentions have been normalized to ISO codes via `norm_languages`, we invoke the coverage routine to quantify how well the parser matches our “ground truth” extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lg = compute_candidate_coverage(\n",
    "    df_cv=df_cv,\n",
    "    df_parser=df_skills,\n",
    "    skill_type=\"Language_Skill\",\n",
    "    extractor=extract_languages,\n",
    "    norm=norm_languages,\n",
    ")\n",
    "\n",
    "print(\"Confusion matrix:\", res_lg.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn = pl.DataFrame(res_lg.fn_rows)\n",
    "sample = df_fn.sample(n=2, shuffle=True)\n",
    "for row in sample.to_dicts():\n",
    "    print_highlighted_cv(row, pattern=languages_pattern_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"False negatives matching snippet pattern: {df_fn.height}\")\n",
    "df_fn.write_csv(LANGUAGE_SKILL_FALSE_NEGATIVES_PATH, separator=\";\")\n",
    "print(\"Saved filtered false negatives to false_negative.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg \n",
    "cd ..\n",
    "# for Unix users\n",
    ".venv/bin/python -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n",
    "\n",
    "# for Windows users\n",
    "#.venv/Scripts/python.exe -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_lg,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"Male\",\n",
    "    group_col=\"Gender\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_lg,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"NORTH\",\n",
    "    group_col=\"Location\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_lg,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"LONG\",\n",
    "    group_col=\"length\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias detection for Job Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading list of jobs from [ESCO](https://esco.ec.europa.eu/en/about-esco) and filtering out those that are too specific (length > 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skills_cleaned = df_skills.with_columns(\n",
    "    pl.col(\"Skill\")\n",
    "    .str.to_lowercase()\n",
    "    .str.replace_all(\"(m/f)\", \"\", literal=True)\n",
    "    .str.strip_chars()\n",
    "    .alias(\"Skill\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **bias detection pipeline for job titles** consists of two main components:\n",
    "\n",
    "* **JobParser**: a class that extracts job experiences listed in [ESCO](https://esco.ec.europa.eu/en/about-esco) from raw CV texts, using SpaCy's `PhraseMatcher`.\n",
    "\n",
    "* **SemanticMatcher**: once job experiences are extracted, the `SemanticMatcher` (`all-MiniLM-L6-v2`) is then used exclusively to determine which of these experiences match the parser extracted skills, employing semantic embeddings. Pairwise cosine similarity is calculated between the embeddings of **JobParser** skills and Parser skills. Matches are established when this similarity exceeds a specified threshold. This matching step is used only to compute metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JobParser(normalized_jobs)\n",
    "matcher = SemanticMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_job = compute_candidate_coverage(\n",
    "    df_cv,\n",
    "    df_skills_cleaned,\n",
    "    \"Job_title\",\n",
    "    parser.parse_with_n_grams,\n",
    "    matcher=matcher.semantic_comparison,\n",
    ")\n",
    "\n",
    "print(\"Confusion matrix:\", res_job.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn = pl.DataFrame(res_job.fn_rows)\n",
    "sample = df_fn.sample(n=2, shuffle=True)\n",
    "for row in sample.to_dicts():\n",
    "    print_highlighted_cv(row, pattern=jobs_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"False negatives matching snippet pattern: {df_fn.height}\")\n",
    "df_fn.write_csv(JOB_TITLE_FALSE_NEGATIVES_PATH, separator=\";\")\n",
    "print(\"Saved filtered false negatives to false_negative.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg \n",
    "cd ..\n",
    "\n",
    "# for Unix users\n",
    ".venv/bin/python -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py\n",
    "\n",
    "# for Windows users\n",
    "#.venv/Scripts/python.exe -m streamlit run hiring_cv_bias/bias_detection/rule_based/app/fn_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_job,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"Male\",\n",
    "    group_col=\"Gender\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_job,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"NORTH\",\n",
    "    group_col=\"Location\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=res_job,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"LONG\",\n",
    "    group_col=\"length\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias detection per skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skills_cleaned = df_skills.with_columns(\n",
    "    pl.col(\"Skill\")\n",
    "    .str.to_lowercase()\n",
    "    .str.replace_all(\"(m/f)\", \"\", literal=True)\n",
    "    .str.strip_chars()\n",
    "    .alias(\"Skill\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JobParser(normalized_jobs)\n",
    "matcher = SemanticMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_job = compute_candidate_coverage(\n",
    "    df_cv,\n",
    "    df_skills_cleaned,\n",
    "    \"Job_title\",\n",
    "    parser.parse_with_n_grams,\n",
    "    matcher=matcher.semantic_comparison,\n",
    ")\n",
    "\n",
    "print(\"Confusion matrix:\", res_job.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ids = set(df_cv[\"CANDIDATE_ID\"].to_list())\n",
    "skills_result = extract_skill_cases(res_job, df_cv, {\"baby-sitter\", \"babysitter\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=skills_result,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"Female\",\n",
    "    group_col=\"Gender\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=skills_result,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"NORTH\",\n",
    "    group_col=\"Location\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=skills_result,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"LONG\",\n",
    "    group_col=\"length\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_jobs_df = load_data(GENDER_JOBS_PATH)\n",
    "gender_jobs_df = gender_jobs_df.filter(pl.col(\"count_female\") > pl.col(\"count_male\"))\n",
    "gender_jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_perc_male = 0\n",
    "avg_perc_female = 0\n",
    "\n",
    "for jobs in gender_jobs_df.iter_rows(named=True):\n",
    "    avg_perc_male += jobs[\"perc_male_zippia\"] * jobs[\"count_total\"]\n",
    "    avg_perc_female += jobs[\"perc_female_zippia\"] * jobs[\"count_total\"]\n",
    "\n",
    "\n",
    "avg_perc_male /= gender_jobs_df[\"count_total\"].sum()\n",
    "avg_perc_female /= gender_jobs_df[\"count_total\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DI expected: {(avg_perc_male / avg_perc_female):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_jobs_df = gender_jobs_df.with_columns(\n",
    "    pl.col(\"Skill\")\n",
    "    .str.to_lowercase()\n",
    "    .str.replace_all(\"(m/f)\", \"\", literal=True)\n",
    "    .str.strip_chars()\n",
    "    .alias(\"Skill\")\n",
    ")\n",
    "gender_jobs = gender_jobs_df[\"Skill\"].to_list()\n",
    "gender_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esco_variants = []\n",
    "for job in gender_jobs_df[\"Skill\"].to_list():\n",
    "    best_score = 0.0\n",
    "    for esco_job in normalized_jobs:\n",
    "        closest = ratio(job, esco_job)\n",
    "        if closest > best_score:\n",
    "            best_score = closest\n",
    "            best_job = esco_job\n",
    "\n",
    "    if best_score > 0.80:\n",
    "        esco_variants.append(best_job)\n",
    "    else:\n",
    "        similarities = matcher.get_similarity(job, normalized_jobs)\n",
    "        if max(similarities[0]) > 0.6:\n",
    "            esco_variants.append(normalized_jobs[np.argmax(similarities.cpu())])\n",
    "\n",
    "esco_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ids = set(df_cv[\"CANDIDATE_ID\"].to_list())\n",
    "skills_result = extract_skill_cases(res_job, df_cv, set(esco_variants + gender_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(\n",
    "    result=skills_result,\n",
    "    df_population=df_cv,\n",
    "    reference_col=\"Female\",\n",
    "    group_col=\"Gender\",\n",
    "    metrics=[\n",
    "        \"equality_of_opportunity\",\n",
    "        \"calibration_npv\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final disparate_impact DI = DI_observed (0.07) / DI_expected (0.16) = 0.44. (<<0.80)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
